{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from zipfile import ZipFile\n",
    "\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "import re\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parq = gpd.read_parquet(\n",
    "    \"/media/muskrat/T7 Shield/eco_data/v3/native/GAP/init/0.parquet\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter by intGapOrigin = 1 (native) (and Origin = Native for double check) and intGapPres = 1 (present) (and Presence = Known/extant for double check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, 1719, 1):\n",
    "    # loading the temp.zip and creating a zip object\n",
    "    with ZipFile(\n",
    "        f\"/media/muskrat/T7 Shield/eco_data/v3/native/GAP/scraped/{i}.zip\", \"r\"\n",
    "    ) as zObject:\n",
    "\n",
    "        # Extracting all the members of the zip\n",
    "        # into a specific location.\n",
    "        zObject.extractall(\n",
    "            path=f\"/media/muskrat/T7 Shield/eco_data/v3/native/GAP/unzipped/{i}\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_list = []\n",
    "for i in range(0, 1719, 1):\n",
    "    #  list the files inside directory {i}\n",
    "    for file_path in os.listdir(\n",
    "        f\"/media/muskrat/T7 Shield/eco_data/v3/native/GAP/unzipped/{i}\"\n",
    "    ):\n",
    "        # print(file_path)\n",
    "        # store each file path before extension in a list\n",
    "\n",
    "        file_split = file_path.split(\".\")\n",
    "        file_list.append(file_split[0])\n",
    "\n",
    "\n",
    "# remove duplicates from file_list and preserve order\n",
    "file_list = list(dict.fromkeys(file_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_df = pd.DataFrame()\n",
    "name_df = pd.DataFrame()\n",
    "geo_df = pd.DataFrame()\n",
    "for i in range(0, 1719, 1):\n",
    "\n",
    "    print(i)\n",
    "    tree = ET.parse(\n",
    "        f\"/media/muskrat/T7 Shield/eco_data/v3/native/GAP/unzipped/{i}/{file_list[i]}.xml\"\n",
    "    )\n",
    "    root = tree.getroot()\n",
    "\n",
    "    name = \"\"\n",
    "    for title in root.iter(\"title\"):\n",
    "        if title.text != None:\n",
    "            # print(title.text)\n",
    "\n",
    "            text = title.text\n",
    "            # find words contained inside parentheses and add to a variable\n",
    "            if (\"(\" in text) and (\")\" in text):\n",
    "                text = re.findall(r\"\\((.*?)\\)\", text)\n",
    "                name = text[0]\n",
    "\n",
    "            else:\n",
    "                print(\"no name\")\n",
    "\n",
    "            break\n",
    "        else:\n",
    "            print(\"no title\")\n",
    "    zip = f\"/media/muskrat/T7 Shield/eco_data/v3/native/GAP/unzipped/{i}/{file_list[i]}.zip\"\n",
    "    with ZipFile(zip, \"r\") as zObject:\n",
    "        # print(zObject.namelist())\n",
    "        # open file in zObject that ends in .csv and save to dataframe\n",
    "        for j in zObject.namelist():\n",
    "            if j.endswith(\".csv\"):\n",
    "                # print(j)\n",
    "                metadata = pd.read_csv(zObject.open(j))\n",
    "                metadata[\"join\"] = i\n",
    "                meta_df = pd.concat([meta_df, metadata])\n",
    "                meta_df = meta_df.drop(columns=\"strHUC12RNG\")\n",
    "                meta_df = meta_df.drop_duplicates()\n",
    "                break\n",
    "\n",
    "    # print(name)\n",
    "\n",
    "    name_df = pd.concat(\n",
    "        [name_df, pd.DataFrame({\"scientific_name\": [name], \"join\": [i]})]\n",
    "    )\n",
    "\n",
    "    geodata = gpd.read_file(zip)\n",
    "    geodata[\"join\"] = i\n",
    "    # drop season code column and season name column from df\n",
    "\n",
    "    geodata = geodata.drop(columns=[\"SeasonCode\", \"SeasonName\"])\n",
    "\n",
    "    geo_df = pd.concat([geo_df, geodata])\n",
    "\n",
    "    # merge meta_df, name_df, and geo_df on join column\n",
    "\n",
    "    df = pd.merge(meta_df, name_df, on=\"join\")\n",
    "\n",
    "    df = pd.merge(df, geo_df, on=\"join\")\n",
    "\n",
    "    df = df.drop(columns=\"join\")\n",
    "\n",
    "    df = gpd.GeoDataFrame(df, geometry=\"geometry\")\n",
    "\n",
    "    # del geo_df, name_df, meta_df, metadata, geodata\n",
    "\n",
    "    # step = 1000\n",
    "    # step_end = len(df)\n",
    "\n",
    "    # if len(df) > step:\n",
    "    #     sub_df = pd.DataFrame()\n",
    "    #     for i in range(0, len(df), step):\n",
    "    #         print(i)\n",
    "    #         if i + step < step_end:\n",
    "    #             sub = df.iloc[i : i + step]\n",
    "    #             sub = sub.drop_duplicates()\n",
    "    #             sub_df = pd.concat([sub_df, sub])\n",
    "    #         else:\n",
    "    #             sub = df.iloc[i:]\n",
    "    #             sub = sub.drop_duplicates()\n",
    "    #             sub_df = pd.concat([sub_df, sub])\n",
    "    #             break\n",
    "\n",
    "    #     sub_df = sub_df.drop_duplicates()\n",
    "    # else:\n",
    "    # df = df.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step = 500\n",
    "step_end = len(df)\n",
    "for i in range(0, len(df), step):\n",
    "    # split df into chunks of size 1000\n",
    "    if i + step < step_end:\n",
    "        sub = df.iloc[i : i + step]\n",
    "        sub.to_parquet(\n",
    "            f\"/media/muskrat/T7 Shield/eco_data/v3/native/GAP/init/{i}.parquet\"\n",
    "        )\n",
    "    else:\n",
    "        sub = df.iloc[i:]\n",
    "        sub.to_parquet(\n",
    "            f\"/media/muskrat/T7 Shield/eco_data/v3/native/GAP/init/{i}.parquet\"\n",
    "        )\n",
    "\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check unique values in scientific_name column in df\n",
    "\n",
    "species_unique = df[\"scientific_name\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del geo_df, name_df, meta_df, metadata, geodata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = sub_df.iloc[0:1]\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot first row of sub_df\n",
    "test.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_2 = sub_df.iloc[3:]\n",
    "test_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_2.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove duplicate rows\n",
    "\n",
    "df = df.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.crs\n",
    "\n",
    "# check units\n",
    "# df.crs.axis_info[0].unit_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converts crs to epsg:4326\n",
    "df = df.to_crs(\"EPSG:4326\")\n",
    "df.crs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ecomap_loc = \"/media/muskrat/T7 Shield/eco_data/ecomap_final/eco_map.geojson\"\n",
    "\n",
    "eco_map = gpd.read_file(ecomap_loc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eco_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eco_map.crs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert df to geodataframe maybe not needed\n",
    "df = gpd.GeoDataFrame(df, geometry=\"geometry\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot ecomap and df on same map\n",
    "base = eco_map.plot(color=\"white\", edgecolor=\"black\")\n",
    "xmin, ymin, xmax, ymax = (-120, 30, -100, 45)\n",
    "\n",
    "ax = df.plot(ax=base, color=\"red\", alpha=0.4)\n",
    "\n",
    "# set the x and y limits of the plot to the specified bounding box coordinates\n",
    "ax.set_xlim(xmin, xmax)\n",
    "ax.set_ylim(ymin, ymax)\n",
    "# plot the GeoDataFrame with the specified bounding box\n",
    "# df.plot(ax=base, color='red', extent=[xmin, xmax, ymin, ymax])\n",
    "\n",
    "# df.plot(ax=base, color='red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if df geometry intersects with ecomap geometry\n",
    "\n",
    "intersects = gpd.sjoin(df, eco_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intersects.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# put unique values of unique_id in intersects into a list\n",
    "\n",
    "unique_ids = list(intersects[\"unique_id\"].unique())\n",
    "unique_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataframe from eco_map that only contains the unique ids in unique_ids\n",
    "\n",
    "eco_map_unique = eco_map[eco_map[\"unique_id\"].isin(unique_ids)]\n",
    "# eco_map_unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eco_map_unique.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overlay = gpd.overlay(df, eco_map, how=\"intersection\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overlay.plot(alpha=0.5, edgecolor=\"k\", cmap=\"tab10\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overlay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "o_area = overlay.area.sum()\n",
    "\n",
    "o_area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add area column to overlay dataframe\n",
    "\n",
    "overlay[\"area\"] = overlay.geometry.area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find row with max area in overlay dataframe\n",
    "\n",
    "overlay.loc[overlay[\"area\"].idxmax()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print overlay row\n",
    "\n",
    "overlay.iloc[6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new dataframe from overlay where the first column is unique_id and the second column is the area of all the rows in overlay that have the same unique_id\n",
    "\n",
    "overlay_areas = overlay[[\"unique_id\", \"area\"]].groupby(\"unique_id\").sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overlay_areas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add an area column to eco_map_unique dataframe\n",
    "\n",
    "eco_map_unique[\"area\"] = eco_map_unique.geometry.area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new dataframe from eco_map_unique where the first column is unique_id and the second column is the area of all the rows in eco_map_unique that have the same unique_id\n",
    "\n",
    "eco_map_unique_areas = eco_map_unique[[\"unique_id\", \"area\"]].groupby(\"unique_id\").sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eco_map_unique_areas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine eco_map_unique_areas and overlay_areas into a new dataframe where the first column is unique_id, the second column is area from overlays, and the third column is area from eco_map_unique\n",
    "\n",
    "combined_areas = pd.concat([overlay_areas, eco_map_unique_areas], axis=1)\n",
    "combined_areas.columns = [\"overlay_area\", \"eco_map_unique_area\"]\n",
    "\n",
    "combined_areas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if overlay_area / eco_map_unique_area > 0.2 then add unique_id to list of ids\n",
    "\n",
    "native = combined_areas[\n",
    "    combined_areas[\"overlay_area\"] / combined_areas[\"eco_map_unique_area\"] > 0.2\n",
    "].index.tolist()\n",
    "\n",
    "native"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inter_repro = intersects.to_crs(\"EPSG:6933\")\n",
    "\n",
    "# inter_repro.crs.axis_info[0].unit_name"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eco",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
