{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "# pd.set_option('display.max_rows', None)\n",
    "# pd.set_option('display.max_columns', None)\n",
    "# pd.set_option('display.width', None)\n",
    "# pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WWF ANIMAL DATA\n",
    "\n",
    "# read in all sheets of ods excel file for animal data\n",
    "df = pd.read_excel('/home/muskrat/Documents/eco_data_copy/main_eco_data/animal_copy.ods',\n",
    "                   index_col=0, sheet_name=None, engine=('odf'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    " \n",
    "# WWF ANIMAL DATA\n",
    "\n",
    "# turn each sheet into dataframe\n",
    "ecoregions = df['ecoregions']\n",
    "ecoregions.reset_index(inplace=True)\n",
    "classes = df['class']\n",
    "common_names = df['common_names']\n",
    "common_names_bup = df['common_names_bup']\n",
    "eco_species = df['ecoregion_species']\n",
    "family = df['family']\n",
    "genus = df['genus']\n",
    "order = df['order_']\n",
    "species = df['species']\n",
    "# add unique id to each ecoregion and convert from int to str\n",
    "ranger = [*range(1, 1 + len(ecoregions))]\n",
    "ecoregion_ids = [str(x) for x in ranger]\n",
    "\n",
    "ecoregions.insert(0, 'unique_id', ecoregion_ids)\n",
    "\n",
    "# print(species.head())\n",
    "# print(genus.head())\n",
    "\n",
    "# df1 = common_names_bup.merge(common_names, how = 'outer' ,indicator=True).loc[lambda x : x['_merge']=='left_only']\n",
    "# df1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    " \n",
    "# WWF ANIMAL DATA\n",
    "\n",
    "# join species and common name dataframes on species id\n",
    "species_common = pd.merge(species, common_names_bup, on='SPECIES_ID', how='left').reindex(\n",
    "    columns=['SPECIES_ID', 'SPECIES', 'COMMON_NAME', 'GENUS_ID'])\n",
    "# find missing value index\n",
    "print(species_common.loc[pd.isna(species_common[\"SPECIES\"]), :].index)\n",
    "\n",
    "# print(species_common.head(10))\n",
    "\n",
    "genus_species = pd.merge(species_common, genus, on='GENUS_ID', how='left').reindex(\n",
    "    columns=['SPECIES_ID', 'GENUS_ID', 'FAMILY_ID', 'GENUS', 'SPECIES', 'COMMON_NAME'])\n",
    "print(genus_species.loc[pd.isna(genus_species[\"GENUS\"]), :].index)\n",
    "\n",
    "family_genus = pd.merge(genus_species, family, on='FAMILY_ID', how='left').reindex(columns=[\n",
    "    'SPECIES_ID', 'GENUS_ID', 'FAMILY_ID', 'ORDER_ID', 'FAMILY', 'GENUS', 'SPECIES', 'COMMON_NAME'])\n",
    "print(family_genus.loc[pd.isna(family_genus[\"FAMILY\"]), :].index)\n",
    "\n",
    "order_family = pd.merge(family_genus, order, on='ORDER_ID', how='left').reindex(columns=[\n",
    "    'SPECIES_ID', 'GENUS_ID', 'FAMILY_ID', 'ORDER_ID', 'CLASS_ID', 'ORDER_DESC', 'FAMILY', 'GENUS', 'SPECIES', 'COMMON_NAME'])\n",
    "print(order_family.loc[pd.isna(order_family[\"ORDER_DESC\"]), :].index)\n",
    "\n",
    "class_order = pd.merge(order_family, classes, on='CLASS_ID', how='left').reindex(columns=[\n",
    "    'SPECIES_ID', 'GENUS_ID', 'FAMILY_ID', 'ORDER_ID', 'CLASS_ID', 'CLASS', 'ORDER_DESC', 'FAMILY', 'GENUS', 'SPECIES', 'COMMON_NAME'])\n",
    "print(class_order.loc[pd.isna(class_order[\"CLASS\"]), :].index)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# WWF ANIMAL DATA\n",
    "\n",
    "# add genus and species columns into one scientific name column\n",
    "class_order['Scientific_Name'] = class_order['GENUS'].str.cat(\n",
    "    class_order['SPECIES'], sep=\" \")\n",
    "# print(class_order.loc[pd.isna(class_order[\"Scientific_Name\"]), :].index)\n",
    "\n",
    "# find all missing values\n",
    "# missing = class_order[class_order.isna().any(axis=1)]\n",
    "\n",
    "# fill missing values with unknown\n",
    "# class_order.fillna('Unknown', inplace=True)\n",
    "\n",
    "eco_species_merged = pd.merge(class_order, eco_species, on='SPECIES_ID', how='left').reindex(columns=[\n",
    "    'SPECIES_ID', 'GENUS_ID', 'FAMILY_ID', 'ORDER_ID', 'CLASS_ID', 'CLASS', 'ORDER_DESC', 'FAMILY', 'GENUS', 'SPECIES', 'Scientific_Name', 'COMMON_NAME', 'ECOREGION_CODE'])\n",
    "\n",
    "eco_species_miss = eco_species_merged.loc[pd.isna(eco_species_merged[\"ECOREGION_CODE\"]), :].index\n",
    "\n",
    "\n",
    "ecoregions_merged = pd.merge(eco_species_merged, ecoregions, on='ECOREGION_CODE', how='left').reindex(columns=[\n",
    "    'SPECIES_ID', 'GENUS_ID', 'FAMILY_ID', 'ORDER_ID', 'CLASS_ID', 'CLASS', 'ORDER_DESC', 'FAMILY', 'GENUS', 'SPECIES', 'Scientific_Name', 'COMMON_NAME', 'ECOREGION_CODE', 'ECOREGION_NAME', 'unique_id'])\n",
    "\n",
    "ecoregions_merged.dropna(subset=['unique_id'], inplace=True)\n",
    " \n",
    "# print(ecos.loc[pd.isna(ecos[\"ECOREGION_NAME\"]), :].index)\n",
    "\n",
    "# ecos.fillna('Unknown', inplace=True)\n",
    "\n",
    "# create a list of all ecoregion names that apply to same species id\n",
    "ecoregion_names_species_mapping = ecoregions_merged.groupby('SPECIES_ID')['ECOREGION_NAME'].apply(list).reset_index()\n",
    "\n",
    "\n",
    "ecoregion_codes_species_mapping = ecoregions_merged.groupby('SPECIES_ID')['ECOREGION_CODE'].apply(list).reset_index()\n",
    "# print(ecoregion_names_species_mapping.head(15))\n",
    "\n",
    "ecoregion_id_species_mapping = ecoregions_merged.groupby('SPECIES_ID')['unique_id'].apply(list).reset_index()\n",
    "\n",
    "final_merge_1 = pd.merge(class_order, ecoregion_names_species_mapping, on='SPECIES_ID', how='left').reindex(columns=[\n",
    "    'SPECIES_ID', 'GENUS_ID', 'FAMILY_ID', 'ORDER_ID', 'CLASS_ID', 'CLASS', 'ORDER_DESC', 'FAMILY', 'GENUS', 'SPECIES', 'Scientific_Name', 'COMMON_NAME', 'ECOREGION_NAME'])\n",
    "\n",
    "final_merge_2 = pd.merge(final_merge_1, ecoregion_codes_species_mapping, on='SPECIES_ID', how='left').reindex(columns=[\n",
    "    'SPECIES_ID', 'GENUS_ID', 'FAMILY_ID', 'ORDER_ID', 'CLASS_ID', 'CLASS', 'ORDER_DESC', 'FAMILY', 'GENUS', 'SPECIES', 'Scientific_Name', 'COMMON_NAME', 'ECOREGION_CODE', 'ECOREGION_NAME'])\n",
    "\n",
    "final_merge_3 = pd.merge(final_merge_2, ecoregion_id_species_mapping, on='SPECIES_ID', how='left').reindex(columns=[\n",
    "    'SPECIES_ID', 'GENUS_ID', 'FAMILY_ID', 'ORDER_ID', 'CLASS_ID', 'CLASS', 'ORDER_DESC', 'FAMILY', 'GENUS', 'SPECIES', 'Scientific_Name', 'COMMON_NAME', 'ECOREGION_CODE', 'unique_id', 'ECOREGION_NAME'])\n",
    "\n",
    "final = final_merge_3[['SPECIES_ID', 'ORDER_DESC', 'FAMILY', 'CLASS', 'Scientific_Name',\n",
    "               'COMMON_NAME', 'ECOREGION_CODE', 'unique_id', 'ECOREGION_NAME']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ecomap = gpd.read_file(\n",
    "    '/media/muskrat/060A9B8E0A9B78FF/eco_data/wwf_terr_map_data/terr_map.geojson')\n",
    "\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "marinemap = gpd.read_file(\n",
    "    '/media/muskrat/060A9B8E0A9B78FF/eco_data/marine_map_data/marine_map.geojson')\n",
    "# marinemap = gpd.read_file(\n",
    "#     '/media/muskrat/060A9B8E0A9B78FF/map_final/eco_map.geojson')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    " \n",
    "\n",
    "\n",
    "ranger = [*range(826, 826+232)]\n",
    "ranger2 = [*range(1058, 1058+37)]\n",
    "ranger3 = ranger2 + ranger\n",
    "marine_ids = [str(x) for x in ranger3]\n",
    "\n",
    "marinemap.insert(0, 'unique_id', marine_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    " \n",
    "\n",
    "marinemap.to_file(\"marine_map.geojson\", driver='GeoJSON')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    " \n",
    "\n",
    "\n",
    "ecomap['name'] = ecomap['ECO_NAME']\n",
    "\n",
    "ecomap[\"unique_id\"] = ecomap[\"unique_id\"].astype(\"float32\")\n",
    "\n",
    "ecomap.drop(ecomap[ecomap['unique_id'].isna()].index, inplace=True)\n",
    "\n",
    "ecomap[\"unique_id\"] = ecomap[\"unique_id\"].astype(\"int32\")\n",
    "\n",
    "ecomap['unique_id'] = ecomap['unique_id'].apply(str)\n",
    "\n",
    "ecomap.drop_duplicates(subset=['unique_id'], keep='first', inplace=True)\n",
    "\n",
    "marinemap['name'] = ''\n",
    "\n",
    "marinemap.loc[marinemap['TYPE'] == 'PPOW', 'name'] = marinemap['PROVINC']\n",
    "\n",
    "marinemap.loc[marinemap['TYPE'] == 'MEOW', 'name'] = marinemap['ECOREGION']\n",
    "\n",
    "econames = pd.concat([ecomap[['unique_id', 'name']], marinemap[[\n",
    "                     'unique_id', 'name']]], axis=0, keys=['unique_id', 'name'])\n",
    "\n",
    "\n",
    "# econames.drop_duplicates(subset=['unique_id'], keep='first', inplace=True)\n",
    "\n",
    "econames.reset_index(drop=True, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    " \n",
    "ecokeys = econames.to_json(orient='records', force_ascii=False)\n",
    "repr(ecokeys)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    " \n",
    "\n",
    "file = open(\"ecoregion_keys.json\", \"w\")\n",
    "file.write(ecokeys)\n",
    "file.close"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# create dataframe of all values in class column that are mammals\n",
    "mammals = final.loc[final['CLASS'].isin(['Mammalia'])].reset_index(drop=True)\n",
    "\n",
    "amphibians = final.loc[final['CLASS'].isin(\n",
    "    ['Amphibia'])].reset_index(drop=True)\n",
    "\n",
    "reptiles = final.loc[final['CLASS'].isin(['Reptilia'])].reset_index(drop=True)\n",
    "\n",
    "birds = final.loc[final['CLASS'].isin(['Aves'])].reset_index(drop=True)\n",
    "\n",
    "# create dataframe of all missing values\n",
    "unknown = final.loc[final['CLASS'].isna()]\n",
    "\n",
    "# create dataframe of all missing common names\n",
    "common_miss = final.loc[final['COMMON_NAME'].isna()]\n",
    "\n",
    "final.drop(final.index[[27729, 27779, 27971]], inplace=True,)\n",
    "final.reset_index(drop=True, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# convert to json\n",
    "mammal_json = mammals.to_json(orient='records', force_ascii=False)\n",
    "repr(mammal_json)\n",
    "\n",
    "amphibian_json = amphibians.to_json(orient='records', force_ascii=False)\n",
    "repr(amphibian_json)\n",
    "\n",
    "reptile_json = reptiles.to_json(orient='records', force_ascii=False)\n",
    "repr(reptile_json)\n",
    "\n",
    "bird_json = birds.to_json(orient='records', force_ascii=False)\n",
    "repr(bird_json)\n",
    "\n",
    "final_json = final.to_json(orient='records', force_ascii=False)\n",
    "repr(final_json)\n",
    "\n",
    "final_miss_json = common_miss.to_json(orient='records', force_ascii=False)\n",
    "repr(final_miss_json)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# WWF ANIMAL DATA\n",
    "\n",
    "# write to files\n",
    "# file = open(\"wwf_mammal.json\", \"w\")\n",
    "# file.write(mammalj)\n",
    "# file.close\n",
    "\n",
    "# file = open(\"wwf_amphibian.json\", \"w\")\n",
    "# file.write(amphibianj)\n",
    "# file.close\n",
    "\n",
    "# file = open(\"wwf_reptile.json\", \"w\")\n",
    "# file.write(reptilej)\n",
    "# file.close\n",
    "\n",
    "# file = open(\"birds_string.json\", \"w\")\n",
    "# file.write(birdj)\n",
    "# file.close\n",
    "\n",
    "file = open(\"wwf_animal_full.json\", \"w\")\n",
    "file.write(final_json)\n",
    "file.close\n",
    "\n",
    "file = open(\"wwf_animal_full_miss.json\", \"w\")\n",
    "file.write(final_miss_json)\n",
    "file.close\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eco",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
