{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from dask import dataframe as dd\n",
    "import dask_geopandas\n",
    "from shapely import wkt\n",
    "\n",
    "\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# INATURALIST DATA\n",
    "\n",
    "df = dd.read_csv('/media/muskrat/060A9B8E0A9B78FF/inat_filtered/0061409-210914110416597.csv', sep='\\t', dtype={'infraspecificEpithet': 'object', 'establishmentMeans': 'object', 'day': 'float64',\n",
    "                                                                                                               'month': 'float64',\n",
    "                                                                                                               'year': 'float64'}, assume_missing=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# %%\n",
    "# df_head = df.head()\n",
    "# df1 = df_head.loc[1,]\n",
    "df.to_parquet('/media/muskrat/060A9B8E0A9B78FF/inat_filtered/park')\n",
    "# names=['kingdom', 'phylum', 'class', 'order', 'family', 'genus', 'species', 'verbatimScientificName', 'decimalLatitude', 'decimalLongitute', 'coordinateUncertaintyInMeters', 'license', 'issue']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# %%\n",
    "\n",
    "# pf = pd.read_csv('/media/muskrat/060A9B8E0A9B78FF/0053270-210914110416597/0053270-210914110416597.csv', nrows=200, index_col=False,  sep='\\t' )\n",
    "\n",
    "dp = dd.read_parquet('/media/muskrat/060A9B8E0A9B78FF/inat_filtered/park', columns=[\n",
    "                     'kingdom', 'phylum', 'class', 'family', 'order', 'verbatimScientificName', 'decimalLatitude', 'decimalLongitude', 'license', 'rightsHolder', 'issue'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# %%\n",
    "\n",
    "# dp_head = dp.head()\n",
    "dp.to_parquet('/media/muskrat/060A9B8E0A9B78FF/inat_filtered/park_rr')\n",
    "# names=['kingdom', 'phylum', 'class', 'order', 'family', 'genus', 'species', 've\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# %%\n",
    "\n",
    "# pp = pd.read_parquet('/media/muskrat/060A9B8E0A9B78FF/0053270-210914110416597/park_reduced')\n",
    "pp = dd.read_parquet('/media/muskrat/060A9B8E0A9B78FF/inat_filtered/park_rr')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# %%\n",
    "\n",
    "# print(pp.loc[dd.isna(pp['decimalLongitude']), :].index)\n",
    "\n",
    "\n",
    "pp = dask_geopandas.from_dask_dataframe(pp)\n",
    "\n",
    "pp = pp.set_geometry(\n",
    "    dask_geopandas.points_from_xy(pp, 'decimalLongitude', 'decimalLatitude')\n",
    ")\n",
    "\n",
    "p_drop = pp.drop(['decimalLatitude', 'decimalLongitude'], axis=1)\n",
    "\n",
    "p_drop = p_drop.compute()\n",
    "\n",
    "df = p_drop\n",
    "\n",
    "del p_drop\n",
    "\n",
    "del pp\n",
    "\n",
    "df.rename(columns={'verbatimScientificName': 'scientific_name'}, inplace=True)\n",
    "\n",
    "df.to_csv('/media/muskrat/060A9B8E0A9B78FF/inat_filtered/man/man.csv')\n",
    "\n",
    "# p_drop.set_crs('epsg:4326')\n",
    "# %%\n",
    "# p_drop.to_parquet('/media/muskrat/060A9B8E0A9B78FF/inat_filtered/park_crc')\n",
    "# gdf = gpd.GeoDataFrame(pp, geometry=gpd.points_from_xy(pp.decimalLongitude, pp.decimalLatitude))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# %%\n",
    "# gpd.options.use_pygeos = False\n",
    "df = pd.read_csv('/media/muskrat/060A9B8E0A9B78FF/inat_filtered/man/man.csv')\n",
    "\n",
    "df = df.drop(['Unnamed: 0'], axis=1)\n",
    "\n",
    "df.reset_index(drop=True, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# %%\n",
    "\n",
    "df[\"kingdom\"] = df[\"kingdom\"].astype(\"category\")\n",
    "\n",
    "df[\"phylum\"] = df[\"phylum\"].astype(\"category\")\n",
    "\n",
    "df[\"class\"] = df[\"class\"].astype(\"category\")\n",
    "\n",
    "df[\"family\"] = df[\"family\"].astype(\"category\")\n",
    "\n",
    "df[\"order\"] = df[\"order\"].astype(\"category\")\n",
    "\n",
    "df[\"license\"] = df[\"license\"].astype(\"category\")\n",
    "\n",
    "df[\"rightsHolder\"] = df[\"rightsHolder\"].astype(\"category\")\n",
    "\n",
    "df[\"issue\"] = df[\"issue\"].astype(\"category\")\n",
    "\n",
    "df[\"scientific_name\"] = df[\"scientific_name\"].astype(\"category\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# %%\n",
    "\n",
    "# missing = df.loc[df.isnull().any(axis=1)]\n",
    "\n",
    "# un = df[df.issue.unique()]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# %%\n",
    "# df = df[ (df['issue'] != 'COORDINATE_ROUNDED;TAXON_MATCH_HIGHERRANK')]\n",
    "# df = df[ (df['issue'] !='TAXON_MATCH_HIGHERRANK')]\n",
    "# df = df[ (df['issue'] !='COORDINATE_ROUNDED;TAXON_MATCH_FUZZY')]\n",
    "# df = df[ (df['issue'] !='TAXON_MATCH_FUZZY')]\n",
    "# df = df[ (df['issue'] !='COORDINATE_ROUNDED;COUNTRY_DERIVED_FROM_COORDINATES;TAXON_MATCH_HIGHERRANK')]\n",
    "# df = df[ (df['issue'] !='COORDINATE_ROUNDED;TAXON_MATCH_HIGHERRANK;MULTIMEDIA_DATE_INVALID')]\n",
    "# df = df[ (df['issue'] !='COORDINATE_ROUNDED;COORDINATE_UNCERTAINTY_METERS_INVALID;TAXON_MATCH_HIGHERRANK')]\n",
    "# df = df[ (df['issue'] !='COORDINATE_ROUNDED;COUNTRY_DERIVED_FROM_COORDINATES;TAXON_MATCH_FUZZY')]\n",
    "# df = df[ (df['issue'] !='COORDINATE_ROUNDED;PRESUMED_SWAPPED_COORDINATE')]\n",
    "# df = df[ (df['issue'] !='COORDINATE_ROUNDED;PRESUMED_NEGATED_LONGITUDE')]\n",
    "# df = df[ (df['issue'] !='COORDINATE_ROUNDED;PRESUMED_NEGATED_LATITUDE')]\n",
    "# df = df[ (df['issue'] !='COORDINATE_UNCERTAINTY_METERS_INVALID;TAXON_MATCH_HIGHERRANK')]\n",
    "# df = df[ (df['issue'] !='COORDINATE_ROUNDED;PRESUMED_SWAPPED_COORDINATE;TAXON_MATCH_HIGHERRANK')]\n",
    "# df = df[ (df['issue'] !='COORDINATE_ROUNDED;COUNTRY_COORDINATE_MISMATCH;TAXON_MATCH_FUZZY')]\n",
    "# df = df[ (df['issue'] !='COORDINATE_ROUNDED;COORDINATE_UNCERTAINTY_METERS_INVALID;TAXON_MATCH_FUZZY')]\n",
    "# df = df[ (df['issue'] !='COUNTRY_DERIVED_FROM_COORDINATES;TAXON_MATCH_HIGHERRANK')]\n",
    "# df = df[ (df['issue'] !='COORDINATE_ROUNDED;COUNTRY_COORDINATE_MISMATCH;TAXON_MATCH_HIGHERRANK')]\n",
    "# df = df[ (df['issue'] !='COUNTRY_COORDINATE_MISMATCH;TAXON_MATCH_HIGHERRANK')]\n",
    "# df = df[ (df['issue'] !='COORDINATE_UNCERTAINTY_METERS_INVALID;TAXON_MATCH_FUZZY')]\n",
    "# df = df[ (df['issue'] !='COUNTRY_DERIVED_FROM_COORDINATES;TAXON_MATCH_FUZZY')]\n",
    "# df = df[ (df['issue'] !='COORDINATE_ROUNDED;PRESUMED_NEGATED_LATITUDE;TAXON_MATCH_FUZZY')]\n",
    "# df = df[ (df['issue'] !='COORDINATE_ROUNDED;TAXON_MATCH_FUZZY;MULTIMEDIA_DATE_INVALID')]\n",
    "# df = df[ (df['issue'] !='COORDINATE_OUT_OF_RANGE')]\n",
    "# df = df[ (df['issue'] !='TAXON_MATCH_HIGHERRANK;MULTIMEDIA_DATE_INVALID')]\n",
    "# df = df[ (df['issue'] !='COUNTRY_COORDINATE_MISMATCH;TAXON_MATCH_FUZZY')]\n",
    "# df = df[ (df['issue'] !='COORDINATE_ROUNDED;COUNTRY_DERIVED_FROM_COORDINATES;TAXON_MATCH_NONE')]\n",
    "# df = df[ (df['issue'] != 'TAXON_MATCH_FUZZY;MULTIMEDIA_DATE_INVALID')]\n",
    "\n",
    "\n",
    "df.drop(df.loc[df['issue'] ==\n",
    "        'COORDINATE_ROUNDED;TAXON_MATCH_HIGHERRANK'].index, inplace=True)\n",
    "df.drop(df.loc[df['issue'] == 'TAXON_MATCH_HIGHERRANK'].index, inplace=True)\n",
    "df.drop(df.loc[df['issue'] ==\n",
    "        'COORDINATE_ROUNDED;TAXON_MATCH_FUZZY'].index, inplace=True)\n",
    "df.drop(df.loc[df['issue'] == 'TAXON_MATCH_FUZZY'].index, inplace=True)\n",
    "df.drop(df.loc[df['issue'] ==\n",
    "        'COORDINATE_ROUNDED;COUNTRY_DERIVED_FROM_COORDINATES;TAXON_MATCH_HIGHERRANK'].index, inplace=True)\n",
    "df.drop(df.loc[df['issue'] ==\n",
    "        'COORDINATE_ROUNDED;TAXON_MATCH_HIGHERRANK;MULTIMEDIA_DATE_INVALID'].index, inplace=True)\n",
    "df.drop(df.loc[df['issue'] == 'COORDINATE_ROUNDED;COORDINATE_UNCERTAINTY_METERS_INVALID;TAXON_MATCH_HIGHERRANK'].index, inplace=True)\n",
    "df.drop(df.loc[df['issue'] ==\n",
    "        'COORDINATE_ROUNDED;COUNTRY_DERIVED_FROM_COORDINATES;TAXON_MATCH_FUZZY'].index, inplace=True)\n",
    "df.drop(df.loc[df['issue'] ==\n",
    "        'COORDINATE_ROUNDED;PRESUMED_SWAPPED_COORDINATE'].index, inplace=True)\n",
    "df.drop(df.loc[df['issue'] ==\n",
    "        'COORDINATE_ROUNDED;PRESUMED_NEGATED_LONGITUDE'].index, inplace=True)\n",
    "df.drop(df.loc[df['issue'] ==\n",
    "        'COORDINATE_ROUNDED;PRESUMED_NEGATED_LATITUDE'].index, inplace=True)\n",
    "df.drop(df.loc[df['issue'] ==\n",
    "        'COORDINATE_UNCERTAINTY_METERS_INVALID;TAXON_MATCH_HIGHERRANK'].index, inplace=True)\n",
    "df.drop(df.loc[df['issue'] ==\n",
    "        'COORDINATE_ROUNDED;PRESUMED_SWAPPED_COORDINATE;TAXON_MATCH_HIGHERRANK'].index, inplace=True)\n",
    "df.drop(df.loc[df['issue'] ==\n",
    "        'COORDINATE_ROUNDED;COUNTRY_COORDINATE_MISMATCH;TAXON_MATCH_FUZZY'].index, inplace=True)\n",
    "df.drop(df.loc[df['issue'] ==\n",
    "        'COORDINATE_ROUNDED;COORDINATE_UNCERTAINTY_METERS_INVALID;TAXON_MATCH_FUZZY'].index, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# %%\n",
    "df.drop(df.loc[df['issue'] ==\n",
    "        'COUNTRY_DERIVED_FROM_COORDINATES;TAXON_MATCH_HIGHERRANK'].index, inplace=True)\n",
    "df.drop(df.loc[df['issue'] ==\n",
    "        'COORDINATE_ROUNDED;COUNTRY_COORDINATE_MISMATCH;TAXON_MATCH_HIGHERRANK'].index, inplace=True)\n",
    "df.drop(df.loc[df['issue'] ==\n",
    "        'COUNTRY_COORDINATE_MISMATCH;TAXON_MATCH_HIGHERRANK'].index, inplace=True)\n",
    "df.drop(df.loc[df['issue'] ==\n",
    "        'COORDINATE_UNCERTAINTY_METERS_INVALID;TAXON_MATCH_FUZZY'].index, inplace=True)\n",
    "df.drop(df.loc[df['issue'] ==\n",
    "        'COUNTRY_DERIVED_FROM_COORDINATES;TAXON_MATCH_FUZZY'].index, inplace=True)\n",
    "df.drop(df.loc[df['issue'] ==\n",
    "        'COORDINATE_ROUNDED;PRESUMED_NEGATED_LATITUDE;TAXON_MATCH_FUZZY'].index, inplace=True)\n",
    "df.drop(df.loc[df['issue'] ==\n",
    "        'COORDINATE_ROUNDED;TAXON_MATCH_FUZZY;MULTIMEDIA_DATE_INVALID'].index, inplace=True)\n",
    "df.drop(df.loc[df['issue'] == 'COORDINATE_OUT_OF_RANGE'].index, inplace=True)\n",
    "df.drop(df.loc[df['issue'] ==\n",
    "        'TAXON_MATCH_HIGHERRANK;MULTIMEDIA_DATE_INVALID'].index, inplace=True)\n",
    "df.drop(df.loc[df['issue'] ==\n",
    "        'COUNTRY_COORDINATE_MISMATCH;TAXON_MATCH_FUZZY'].index, inplace=True)\n",
    "df.drop(df.loc[df['issue'] ==\n",
    "        'COORDINATE_ROUNDED;COUNTRY_DERIVED_FROM_COORDINATES;TAXON_MATCH_NONE'].index, inplace=True)\n",
    "df.drop(df.loc[df['issue'] ==\n",
    "        'TAXON_MATCH_FUZZY;MULTIMEDIA_DATE_INVALID'].index, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# %%\n",
    "\n",
    "df.reset_index(drop=True, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# %%\n",
    "df.to_csv('/media/muskrat/060A9B8E0A9B78FF/inat_filtered/geo/geo.csv', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# %%\n",
    "\n",
    "df = pd.read_csv('/media/muskrat/060A9B8E0A9B78FF/inat_filtered/geo/geo.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# %%\n",
    "\n",
    "df[\"kingdom\"] = df[\"kingdom\"].astype(\"category\")\n",
    "\n",
    "df[\"phylum\"] = df[\"phylum\"].astype(\"category\")\n",
    "\n",
    "df[\"class\"] = df[\"class\"].astype(\"category\")\n",
    "\n",
    "df[\"license\"] = df[\"license\"].astype(\"category\")\n",
    "\n",
    "df[\"rightsHolder\"] = df[\"rightsHolder\"].astype(\"category\")\n",
    "\n",
    "df[\"issue\"] = df[\"issue\"].astype(\"category\")\n",
    "\n",
    "df[\"scientific_name\"] = df[\"scientific_name\"].astype(\"category\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# %%\n",
    "# pp = dask_geopandas.from_dask_dataframe(df)\n",
    "\n",
    "# pp = pp[(pp['coordinateUncertaintyInMeters'] <= 400)]\n",
    "\n",
    "# pp = pp.drop(['coordinateUncertaintyInMeters', 'issue'], axis=1)\n",
    "\n",
    "# pp.to_parquet('/media/muskrat/060A9B8E0A9B78FF/0053270-210914110416597/man')\n",
    "\n",
    "# # dp = pp[(pp[pp['geometry'].is_empty])]\n",
    "# # df = df[(df['issue'] != 'TAXON_MATCH_FUZZY;MULTIMEDIA_DATE_INVALID')]\n",
    "\n",
    "# df.drop(df[df['coordinateUncertaintyInMeters'] >= 400].index, inplace=True)\n",
    "\n",
    "# df = df.drop(['coordinateUncertaintyInMeters', 'issue'], axis=1)\n",
    "\n",
    "df['geometry'] = df['geometry'].apply(wkt.loads)\n",
    "\n",
    "df = gpd.GeoDataFrame(df, crs='epsg:4326')\n",
    "\n",
    "df.drop(df[df['geometry'].is_empty].index, inplace=True)\n",
    "\n",
    "\n",
    "# df.drop(df[df['coordinateUncertaintyInMeters'] >= 400].index, inplace=True)\n",
    "\n",
    "# df = df.drop(['coordinateUncertaintyInMeters', 'issue'], axis=1)\n",
    "# %%\n",
    "\n",
    "# dd = pd.read_parquet('/media/muskrat/060A9B8E0A9B78FF/0053270-210914110416597/man')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# %%\n",
    "# df_head = df.head(n=10)\n",
    "\n",
    "# df_tail = df.tail(n=1000)\n",
    "\n",
    "# unique = df.drop_duplicates(['verbatimScientificName'])\n",
    "\n",
    "rights = ['license', 'rightsHolder']\n",
    "df['rights'] = df[rights].to_dict(orient='records')\n",
    "df = df.drop(['license', 'rightsHolder', 'issue'], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# %%\n",
    "# print(df.loc[pd.isna(df['geometry']), :].index)\n",
    "\n",
    "\n",
    "df.reset_index(drop=True, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# %%\n",
    "\n",
    "# df.set_crs('EPSG:4326', inplace=True)\n",
    "\n",
    "df.to_parquet(\n",
    "    '/media/muskrat/060A9B8E0A9B78FF/inat_filtered/park_geo/park_geo.parquet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# %%\n",
    "df = gpd.read_parquet(\n",
    "    '/media/muskrat/060A9B8E0A9B78FF/inat_filtered/park_geo/park_geo.parquet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# %%\n",
    "ecomap = gpd.read_file(\n",
    "    '/home/muskrat/Documents/eco_data_copy/wwf_map_data/map.geojson')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# %%\n",
    "# head = df.head(n=1000)\n",
    "\n",
    "ecomap = ecomap.drop(['OBJECTID', 'AREA', 'PERIMETER', 'REALM', 'BIOME', 'ECO_NUM', 'ECO_NAME', 'ECO_ID', 'ECO_SYM', 'GBL_STAT', 'G200_REGIO',\n",
    "                     'G200_NUM', 'G200_BIOME', 'G200_STAT', 'Shape_Leng', 'Shape_Area', 'area_km2', 'ECOREGION_CODE', 'PER_area', 'PER_area_1', 'PER_area_2'], axis=1)\n",
    "\n",
    "ecomap[\"unique_id\"] = ecomap[\"unique_id\"].astype(\"float32\")\n",
    "\n",
    "ecomap.drop(ecomap[ecomap['unique_id'].isna()].index, inplace=True)\n",
    "\n",
    "ecomap[\"unique_id\"] = ecomap[\"unique_id\"].astype(\"int32\")\n",
    "\n",
    "ecomap['unique_id'] = ecomap['unique_id'].apply(str)\n",
    "\n",
    "ecomap[\"unique_id\"] = ecomap[\"unique_id\"].astype(\"category\")\n",
    "\n",
    "# %%\n",
    "# ecomap.loc[4887, 'geometry'].contains(head.loc[5, 'geometry'])\n",
    "\n",
    "# test = ecomap.sjoin(head, predicate='contains')\n",
    "# test2 = head.sjoin(ecomap, predicate='contains')\n",
    "# test3 = head.sjoin(ecomap, predicate='within')\n",
    "# %%\n",
    "# test3 = test3.drop(['index_right', 'ECO_NAME'], axis=1)\n",
    "\n",
    "# ecos3 = test3.groupby('scientific_name')['unique_id'].apply(list).reset_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# %%\n",
    "\n",
    "joined = df.sjoin(ecomap, predicate='within')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# %%\n",
    "joined = joined.drop(['index_right', 'geometry'], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# %%\n",
    "ecos3 = joined.groupby('scientific_name')[\n",
    "    'unique_id'].apply(list).reset_index()\n",
    "\n",
    "ecos4 = joined.groupby('scientific_name')['rights'].apply(list).reset_index()\n",
    "\n",
    "# ecos5 = joined.groupby(['scientific_name'], as_index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# %%\n",
    "# ecos5 = ecos5.apply(lambda x: x)\n",
    "ecos7 = joined.drop_duplicates(subset=['scientific_name'])\n",
    "ecos7 = ecos7.drop(['unique_id', 'rights'], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# %%\n",
    "\n",
    "ecos6 = pd.merge(ecos3, ecos4, on='scientific_name', how='left').reindex(columns=[\n",
    "    'scientific_name', 'unique_id', 'rights'])\n",
    "\n",
    "\n",
    "final = pd.merge(ecos6, ecos7, on='scientific_name', how='left')\n",
    "\n",
    "final1 = final[final['unique_id'].map(lambda d: len(d)) > 0]\n",
    "\n",
    "final2 = final1[['kingdom', 'phylum', 'class', 'family',\n",
    "                 'order', 'scientific_name', 'unique_id', 'rights']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# %%\n",
    "\n",
    "finalj = final2.to_json(orient='records', force_ascii=False)\n",
    "repr(finalj)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# %%\n",
    "\n",
    "file = open(\"inat_full.json\", \"w\")\n",
    "file.write(finalj)\n",
    "file.close"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
