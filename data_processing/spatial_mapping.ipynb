{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 400 meter coordinate accuracy\n",
    "# 1923 earliest year\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from dask import dataframe as dd\n",
    "import dask_geopandas\n",
    "\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "# pd.set_option('display.max_rows', None)\n",
    "# pd.set_option('display.max_columns', None)\n",
    "# pd.set_option('display.width', None)\n",
    "# pd.set_option('display.max_colwidth', None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dask visualizations\n",
    "from dask.distributed import Client\n",
    "\n",
    "client = Client()  # start distributed scheduler locally.\n",
    "client\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in cleaned parquet\n",
    "df_path = input(\"Enter the file path: \")\n",
    "\n",
    "df = dd.read_parquet(df_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in eco_map.geojson\n",
    "map_path = input(\"Enter the file path: \")\n",
    "\n",
    "map = gpd.read_file(map_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map.crs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to dask_geopandas df\n",
    "df_gpd = dask_geopandas.from_dask_dataframe(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df, df_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gpd = df_gpd.set_geometry(\n",
    "    dask_geopandas.points_from_xy(\n",
    "        df_gpd, \"decimalLongitude\", \"decimalLatitude\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gpd = df_gpd.drop([\"decimalLatitude\", \"decimalLongitude\"], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gpd = df_gpd.set_crs(\"EPSG:4326\")\n",
    "\n",
    "# if get stuck might be useful at some point\n",
    "# df_gpd.drop(df_gpd[df_gpd[\"geometry\"].is_empty].index, inplace=True)\n",
    "\n",
    "# df_gpd[\"geometry\"] = df_gpd[\"geometry\"].apply(wkt.loads)\n",
    "# df_gpd = gpd.GeoDataFrame(df_gpd, crs=\"EPSG:4326\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gpd.crs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gpd = df_gpd.compute()\n",
    "\n",
    "# df_gpd.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gpd = df_gpd.sjoin(map, predicate=\"within\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gpd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del map, map_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gpd = df_gpd.drop([\"index_right\", \"geometry\", \"name\", \"TYPE\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gpd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine license and rightsHolder into dict and merge in single column\n",
    "rights = [\"license\", \"rightsHolder\"]\n",
    "df_gpd[\"rights\"] = df_gpd[rights].to_dict(orient=\"records\")\n",
    "df_gpd = df_gpd.drop([\"license\", \"rightsHolder\"], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gpd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_id = df_gpd.groupby(\"species\")[\"unique_id\"].apply(list).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group_rights = df_gpd.groupby(\"species\")[\"rights\"].apply(\n",
    "#     # create empty list and only dictionary values that are not already in list\n",
    "#     list\n",
    "#     ).apply(lambda x: [i for n, i in enumerate(x) if i not in x[n + 1:]]\n",
    "\n",
    "\n",
    "#     ).reset_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_rights = df_gpd.groupby(\"species\")[\"rights\"].apply(list).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ecos5 = df_gpd.groupby(['scientific_name'], as_index=False)\n",
    "\n",
    "\n",
    "# %%\n",
    "\n",
    "\n",
    "# ecos5 = ecos5.apply(lambda x: x)\n",
    "unique_species = df_gpd.drop_duplicates(subset=[\"species\"])\n",
    "unique_species = unique_species.drop([\"unique_id\", \"rights\"], axis=1)\n",
    "\n",
    "\n",
    "# %%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df_gpd, rights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_groups = pd.merge(group_id, group_rights, on=\"species\", how=\"left\").reindex(\n",
    "    columns=[\"species\", \"unique_id\", \"rights\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del group_id, group_rights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_merge = pd.merge(merged_groups, unique_species, on=\"species\", how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del merged_groups, unique_species"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove duplicates values from list in unique_id column\n",
    "total_merge[\"unique_id\"] = total_merge[\"unique_id\"].apply(set).apply(list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_merge\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rights = total_merge[\"rights\"].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tester of removing duplicate dicts from list\n",
    "# rights_43 = rights[43]\n",
    "\n",
    "# only keep unique values from rights_43 list\n",
    "# rights_43 = [i for n, i in enumerate(rights_43) if i not in rights_43[n + 1:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VERY SLOW!!! TOOK 4 HOURS ON 8500 ROWS(10 MILLION COMPRESSED)\n",
    "# remove duplicate dictionaries from list of lists\n",
    "for j in range(len(rights)):\n",
    "    print(j)\n",
    "    rights[j] = [i for n, i in enumerate(\n",
    "        rights[j]) if i not in rights[j][n + 1:]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check some rights values. doesn't show up properly in variable explorer\n",
    "rights[103]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to series\n",
    "rights = pd.Series(rights, name=\"rights\")\n",
    "\n",
    "total_merge[\"rights\"] = rights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del rights, j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename species column to scientific_name\n",
    "total_merge = total_merge.rename(columns={\"species\": \"scientific_name\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# REMOVE \"<NA>\" VALUES FROM UNIQUE_ID COLUMN\n",
    "\n",
    "# df['unique_id'].apply(lambda x: print(x) if ('<NA>' in x) else x)\n",
    "\n",
    "# df[\"unique_id\"] = df[\"unique_id\"].apply(\n",
    "#     lambda x: x[x != \"<NA>\"] if (\"<NA>\" in x) else x\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_merge = total_merge[\n",
    "    [\n",
    "        \"kingdom\",\n",
    "        \"phylum\",\n",
    "        \"class\",\n",
    "        \"order\",\n",
    "        \"family\",\n",
    "        \"genus\",\n",
    "        \"scientific_name\",\n",
    "        \"unique_id\",\n",
    "        \"rights\",\n",
    "    ]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_merge\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read cleaned df into a parquet file where user can input the file path\n",
    "df_spatial_path = input(\"Enter the file path: \")\n",
    "\n",
    "\n",
    "# write df to parquet file using pandas to_parquet\n",
    "total_merge.to_parquet(df_spatial_path)\n",
    "\n",
    "del total_merge, df_spatial_path\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eco",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
