{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "# pd.set_option('display.max_rows', 100)\n",
    "# pd.set_option('display.max_seq_items', 100)\n",
    "\n",
    "# 400 meter coordinate accuracy\n",
    "# 1924 earliest year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_path = input(\"Enter the file path: \")\n",
    "df = pd.read_parquet(df_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checklist_path = input(\"Enter the checklist path: \")\n",
    "checklist = pd.read_parquet(checklist_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checklist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset index on df and checklist\n",
    "df = df.reset_index(drop=True)\n",
    "checklist = checklist.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicates_df = df[df.duplicated([\"scientific_name\"], keep=False)]\n",
    "duplicates_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename canonicalName to scientific_name in checklist\n",
    "checklist.rename(columns={\"canonicalName\": \"scientific_name\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicates_checklist = checklist[checklist.duplicated([\"scientific_name\"], keep=False)]\n",
    "duplicates_checklist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_in_checklist = df[df[\"scientific_name\"].isin(checklist[\"scientific_name\"])]\n",
    "df_in_checklist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_not_in_checklist = df[~df[\"scientific_name\"].isin(checklist[\"scientific_name\"])]\n",
    "df_not_in_checklist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_in_duplicates_checklist = df[\n",
    "    df[\"scientific_name\"].isin(duplicates_checklist[\"scientific_name\"])\n",
    "]\n",
    "df_in_duplicates_checklist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean = pd.merge(\n",
    "    df,\n",
    "    checklist,\n",
    "    how=\"inner\",\n",
    "    on=[\"kingdom\", \"phylum\", \"class\", \"order\", \"family\", \"genus\", \"scientific_name\"],\n",
    ")\n",
    "df_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_dirty equals rows of df not in df_cleean\n",
    "df_dirty = df[~df.index.isin(df_clean.index)]\n",
    "df_dirty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop columns taxonRank and taxonomicStatus\n",
    "df_clean = df_clean.drop([\"taxonRank\", \"taxonomicStatus\"], axis=1)\n",
    "df_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill missing values of native_ecoregions and freshwater_ecoregions with empty list\n",
    "for i in range(len(df_clean)):\n",
    "    print(i)\n",
    "    #    print(type(df_clean.loc[i, \"native_ecoregions\"]))\n",
    "    if df_clean.loc[i, \"native_ecoregions\"] is None:\n",
    "        df_clean.loc[i, \"native_ecoregions\"] = [None]\n",
    "    if df_clean.loc[i, \"freshwater_ecoregions\"] is None:\n",
    "        df_clean.loc[i, \"freshwater_ecoregions\"] = [None]\n",
    "\n",
    "\n",
    "df_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean = df_clean.groupby([\"acceptedName\"], as_index=False, dropna=False).apply(\n",
    "    lambda x: pd.Series(\n",
    "        {\n",
    "            \"observed_ecoregions\": list(\n",
    "                [item for sublist in x[\"observed_ecoregions\"] for item in sublist]\n",
    "            ),\n",
    "            \"rights\": list([item for sublist in x[\"rights\"] for item in sublist]),\n",
    "            \"native_ecoregions\": list(\n",
    "                set(\n",
    "                    [\n",
    "                        item\n",
    "                        for sublist in x[\"native_ecoregions\"]\n",
    "                        for item in sublist\n",
    "                        if item is not None\n",
    "                    ]\n",
    "                )\n",
    "            ),\n",
    "            \"freshwater_ecoregions\": list(\n",
    "                set(\n",
    "                    [\n",
    "                        item\n",
    "                        for sublist in x[\"freshwater_ecoregions\"]\n",
    "                        for item in sublist\n",
    "                        if item is not None\n",
    "                    ]\n",
    "                )\n",
    "            ),\n",
    "            \"common_name\": list(x[\"common_name\"].unique()),\n",
    "            \"species_type\": list(x[\"species_type\"].unique()),\n",
    "            \"scientific_name\": list(x[\"scientific_name\"].unique()),\n",
    "            \"kingdom\": list(x[\"kingdom\"].unique()),\n",
    "            \"phylum\": list(x[\"phylum\"].unique()),\n",
    "            \"class\": list(x[\"class\"].unique()),\n",
    "            \"order\": list(x[\"order\"].unique()),\n",
    "            \"family\": list(x[\"family\"].unique()),\n",
    "            \"genus\": list(x[\"genus\"].unique()),\n",
    "        }\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find rows of df_clean where length of scientific_name is greater than 1\n",
    "scientific_dups = df_clean[df_clean[\"scientific_name\"].str.len() > 1]\n",
    "\n",
    "scientific_dups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kingdom_dups = df_clean[df_clean[\"kingdom\"].str.len() > 1]\n",
    "\n",
    "kingdom_dups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phylum_dups = df_clean[df_clean[\"phylum\"].str.len() > 1]\n",
    "\n",
    "phylum_dups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_dups = df_clean[df_clean[\"class\"].str.len() > 1]\n",
    "\n",
    "class_dups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "order_dups = df_clean[df_clean[\"order\"].str.len() > 1]\n",
    "\n",
    "order_dups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "family_dups = df_clean[df_clean[\"family\"].str.len() > 1]\n",
    "\n",
    "family_dups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genus_dups = df_clean[df_clean[\"genus\"].str.len() > 1]\n",
    "\n",
    "genus_dups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean = df_clean[~df_clean.index.isin(kingdom_dups.index)]\n",
    "\n",
    "df_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean = df_clean.reset_index(drop=True)\n",
    "\n",
    "df_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "species_type_dups = df_clean[df_clean[\"species_type\"].str.len() > 1]\n",
    "\n",
    "species_type_dups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(df_clean)):\n",
    "    print(i)\n",
    "    df_clean.loc[i, \"kingdom\"] = df_clean.loc[i, \"kingdom\"][0]\n",
    "    df_clean.loc[i, \"phylum\"] = df_clean.loc[i, \"phylum\"][0]\n",
    "    df_clean.loc[i, \"class\"] = df_clean.loc[i, \"class\"][0]\n",
    "    df_clean.loc[i, \"order\"] = df_clean.loc[i, \"order\"][0]\n",
    "    df_clean.loc[i, \"family\"] = df_clean.loc[i, \"family\"][0]\n",
    "    df_clean.loc[i, \"genus\"] = df_clean.loc[i, \"genus\"][0]\n",
    "    df_clean.loc[i, \"species_type\"] = df_clean.loc[i, \"species_type\"][0]\n",
    "\n",
    "\n",
    "df_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_name_dups = df_clean[df_clean[\"common_name\"].str.len() > 1]\n",
    "\n",
    "common_name_dups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean[\"common_name\"] = df_clean[\"common_name\"].apply(\n",
    "    lambda x: [i for i in x if i is not None]\n",
    ")\n",
    "\n",
    "df_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(df_clean)):\n",
    "    print(i)\n",
    "    if len(df_clean.loc[i, \"common_name\"]) == 0:\n",
    "        df_clean.loc[i, \"common_name\"] = None\n",
    "    else:\n",
    "        df_clean.loc[i, \"common_name\"] = df_clean.loc[i, \"common_name\"][0]\n",
    "\n",
    "df_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop column scientific_name\n",
    "df_clean = df_clean.drop([\"scientific_name\"], axis=1)\n",
    "\n",
    "df_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename acceptedName to scientific_name\n",
    "df_clean = df_clean.rename(columns={\"acceptedName\": \"scientific_name\"})\n",
    "\n",
    "df_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'max ecoregions length is {df_clean[\"observed_ecoregions\"].str.len().max()}')\n",
    "print(f'max rights length is {df_clean[\"rights\"].str.len().max()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(df_clean)):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SLOW, TOOK ABOUT AN HOUR FOR 300,000+ ROWS\n",
    "for i in range(len(df_clean)):\n",
    "    print(f\"index: {i}\")\n",
    "    # print(len(test[\"observed_ecoregions\"].iloc[i]))\n",
    "    cat = []\n",
    "    rights_cat = []\n",
    "    for j in range(len(df_clean[\"observed_ecoregions\"].iloc[i])):\n",
    "        if df_clean[\"observed_ecoregions\"].iloc[i][j] not in cat:\n",
    "            cat.append(df_clean[\"observed_ecoregions\"].iloc[i][j])\n",
    "            rights_cat.append(df_clean[\"rights\"].iloc[i][j])\n",
    "        # print(test[\"observed_ecoregions\"].iloc[i][j])\n",
    "        # for k in test[\"observed_ecoregions\"].iloc[i][j]:\n",
    "        # print(k)\n",
    "        # if k not in cat:\n",
    "        #     cat.append(k)\n",
    "    # if len(cat) != len(test[\"observed_ecoregions\"].iloc[i]):\n",
    "    # print(f'index: {i}')\n",
    "\n",
    "    # print(f'observed: {test[\"observed_ecoregions\"].iloc[i]}')\n",
    "    # print(f'cat: {cat}')\n",
    "    if len(df_clean[\"rights\"].iloc[i]) != len(df_clean[\"observed_ecoregions\"].iloc[i]):\n",
    "        print(f\"index: {i}\")\n",
    "        print(\"rights\")\n",
    "\n",
    "    df_clean[\"observed_ecoregions\"].iloc[i] = cat\n",
    "    df_clean[\"rights\"].iloc[i] = rights_cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean.to_parquet(\n",
    "    \"/media/muskrat/T7 Shield/eco_data/v4/occurences/step_4/processing/step_3/checklist/clean_checklisted.parquet\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dirty.to_parquet(\n",
    "    \"/media/muskrat/T7 Shield/eco_data/v4/occurences/step_4/processing/step_3/checklist/dirty_checklisted.parquet\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eco",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
