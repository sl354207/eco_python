{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "# pd.set_option('display.max_rows', 100)\n",
    "# pd.set_option('display.max_seq_items', 100)\n",
    "\n",
    "# 400 meter coordinate accuracy\n",
    "# 1923 earliest year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in new merged gbif_obis data\n",
    "new_path = input(\"Enter the file path: \")\n",
    "\n",
    "new = pd.read_parquet(new_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in old species data\n",
    "old_path = input(\"Enter the file path: \")\n",
    "\n",
    "# read in the old species data\n",
    "old = pd.read_parquet(old_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "old\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "species = old[\"scientific_name\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for strange x at beginning of species names and remove if necessary\n",
    "\n",
    "# old = old[\n",
    "#     ~old.scientific_name.isin([\"× Heucherella\", \"×Elyhordeum schaackianum\", \"×Elyleymus aleuticus\"])\n",
    "# ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for species names that longer than 2 words and remove if necessary\n",
    "species_values = np.array([len(word.split()) for word in species])\n",
    "\n",
    "# find the max value in species_values\n",
    "max_value = np.max(species_values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if value at index i in species_values is greater than 2, add the value at the same index in species_unique to a new list\n",
    "species_2 = []\n",
    "for i in range(len(species_values)):\n",
    "    if species_values[i] > 2:\n",
    "        species_2.append(species[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove values from species that are greater than 2\n",
    "\n",
    "for i in range(len(species_2)):\n",
    "    old = old[old.scientific_name != species_2[i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del species_2, species_values, max_value, i, species\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inner = pd.merge(new, old, how=\"inner\", on=[\"scientific_name\"], indicator=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inner[\"combined_id\"] = inner[\"unique_id_x\"].apply(lambda x: x.tolist()) + inner[\n",
    "    \"unique_id_y\"\n",
    "].apply(lambda x: x.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inner[\"combined_id\"] = inner[\"combined_id\"].apply(set).apply(list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_combined_id = inner[[\"unique_id_x\", \"unique_id_y\", \"combined_id\"]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inner = inner.drop([\"unique_id_x\", \"unique_id_y\"], axis=1)\n",
    "\n",
    "inner.rename(columns={\"combined_id\": \"unique_id\"}, inplace=True)\n",
    "\n",
    "del check_combined_id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inner[\"combined_rights\"] = inner[\"rights_x\"].apply(lambda x: x.tolist()) + inner[\n",
    "    \"rights_y\"\n",
    "].apply(lambda x: x.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check combined_rights\n",
    "print(\n",
    "    f\"right {inner['rights_x'][0]}, left {inner['rights_y'][0]}, combined {inner['combined_rights'][0]}\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove duplicates\n",
    "rights = inner[\"combined_rights\"].to_list()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in range(len(rights)):\n",
    "    print(j)\n",
    "    rights[j] = [i for n, i in enumerate(rights[j]) if i not in rights[j][n + 1 :]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rights = pd.Series(rights, name=\"rights\")\n",
    "\n",
    "inner[\"combined_rights\"] = rights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recheck combined_rights\n",
    "print(\n",
    "    f\"right {inner['rights_x'][493]}, left {inner['rights_y'][493]}, combined {inner['combined_rights'][493]}\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inner = inner.drop([\"rights_x\", \"rights_y\"], axis=1)\n",
    "\n",
    "inner.rename(columns={\"combined_rights\": \"rights\"}, inplace=True)\n",
    "\n",
    "del rights, j\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep columns with less missing values\n",
    "\n",
    "kingdom_x_missing = inner[\"kingdom_x\"].isna().sum()\n",
    "\n",
    "kingdom_y_missing = inner[\"kingdom_y\"].isna().sum()\n",
    "\n",
    "# drop the kingdom column from inner that contains more missing values\n",
    "if kingdom_x_missing > kingdom_y_missing:\n",
    "    inner = inner.drop([\"kingdom_x\"], axis=1)\n",
    "    inner.rename(columns={\"kingdom_y\": \"kingdom\"}, inplace=True)\n",
    "else:\n",
    "    inner = inner.drop([\"kingdom_y\"], axis=1)\n",
    "    inner.rename(columns={\"kingdom_x\": \"kingdom\"}, inplace=True)\n",
    "\n",
    "phylum_x_missing = inner[\"phylum_x\"].isna().sum()\n",
    "\n",
    "phylum_y_missing = inner[\"phylum_y\"].isna().sum()\n",
    "\n",
    "# drop the phylum column from inner that contains more missing values\n",
    "\n",
    "if phylum_x_missing > phylum_y_missing:\n",
    "    inner = inner.drop([\"phylum_x\"], axis=1)\n",
    "    inner.rename(columns={\"phylum_y\": \"phylum\"}, inplace=True)\n",
    "else:\n",
    "    inner = inner.drop([\"phylum_y\"], axis=1)\n",
    "    inner.rename(columns={\"phylum_x\": \"phylum\"}, inplace=True)\n",
    "\n",
    "# find sum of missing values in inner['class_x']\n",
    "class_x_missing = inner[\"class_x\"].isna().sum()\n",
    "\n",
    "class_y_missing = inner[\"class_y\"].isna().sum()\n",
    "\n",
    "# drop the class column from inner that contains more missing values\n",
    "if class_x_missing > class_y_missing:\n",
    "    inner = inner.drop([\"class_x\"], axis=1)\n",
    "    inner.rename(columns={\"class_y\": \"class\"}, inplace=True)\n",
    "else:\n",
    "    inner = inner.drop([\"class_y\"], axis=1)\n",
    "    inner.rename(columns={\"class_x\": \"class\"}, inplace=True)\n",
    "\n",
    "# check\n",
    "class_missing = inner[\"class\"].isna().sum()\n",
    "\n",
    "order_x_missing = inner[\"order_x\"].isna().sum()\n",
    "\n",
    "order_y_missing = inner[\"order_y\"].isna().sum()\n",
    "\n",
    "# drop the order column from inner that contains more missing values\n",
    "\n",
    "if order_x_missing > order_y_missing:\n",
    "    inner = inner.drop([\"order_x\"], axis=1)\n",
    "    inner.rename(columns={\"order_y\": \"order\"}, inplace=True)\n",
    "else:\n",
    "    inner = inner.drop([\"order_y\"], axis=1)\n",
    "    inner.rename(columns={\"order_x\": \"order\"}, inplace=True)\n",
    "\n",
    "family_x_missing = inner[\"family_x\"].isna().sum()\n",
    "\n",
    "family_y_missing = inner[\"family_y\"].isna().sum()\n",
    "\n",
    "# drop the family column from inner that contains more missing values\n",
    "\n",
    "if family_x_missing > family_y_missing:\n",
    "    inner = inner.drop([\"family_x\"], axis=1)\n",
    "    inner.rename(columns={\"family_y\": \"family\"}, inplace=True)\n",
    "else:\n",
    "    inner = inner.drop([\"family_y\"], axis=1)\n",
    "    inner.rename(columns={\"family_x\": \"family\"}, inplace=True)\n",
    "\n",
    "\n",
    "genus_x_missing = inner[\"genus_x\"].isna().sum()\n",
    "\n",
    "genus_y_missing = inner[\"genus_y\"].isna().sum()\n",
    "\n",
    "# drop the genus column from inner that contains more missing values\n",
    "\n",
    "if genus_x_missing > genus_y_missing:\n",
    "    inner = inner.drop([\"genus_x\"], axis=1)\n",
    "    inner.rename(columns={\"genus_y\": \"genus\"}, inplace=True)\n",
    "else:\n",
    "    inner = inner.drop([\"genus_y\"], axis=1)\n",
    "    inner.rename(columns={\"genus_x\": \"genus\"}, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del (\n",
    "    genus_y_missing,\n",
    "    genus_x_missing,\n",
    "    family_y_missing,\n",
    "    family_x_missing,\n",
    "    order_y_missing,\n",
    "    order_x_missing,\n",
    "    class_y_missing,\n",
    "    class_x_missing,\n",
    "    phylum_y_missing,\n",
    "    phylum_x_missing,\n",
    "    kingdom_y_missing,\n",
    "    kingdom_x_missing,\n",
    "    class_missing,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop _merge column\n",
    "inner = inner.drop(\"_merge\", axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inner = inner[\n",
    "    [\n",
    "        \"kingdom\",\n",
    "        \"phylum\",\n",
    "        \"class\",\n",
    "        \"order\",\n",
    "        \"family\",\n",
    "        \"genus\",\n",
    "        \"scientific_name\",\n",
    "        \"common_name\",\n",
    "        \"species_type\",\n",
    "        \"unique_id\",\n",
    "        \"rights\",\n",
    "    ]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outer = pd.merge(new, old, how=\"outer\", on=[\"scientific_name\"], indicator=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "left = outer.loc[\n",
    "    outer._merge == \"left_only\",\n",
    "    [\n",
    "        \"kingdom_x\",\n",
    "        \"phylum_x\",\n",
    "        \"class_x\",\n",
    "        \"order_x\",\n",
    "        \"family_x\",\n",
    "        \"genus_x\",\n",
    "        \"scientific_name\",\n",
    "        \"common_name\",\n",
    "        \"species_type\",\n",
    "        \"unique_id_x\",\n",
    "        \"rights_x\",\n",
    "    ],\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "left.rename(\n",
    "    columns={\n",
    "        \"kingdom_x\": \"kingdom\",\n",
    "        \"phylum_x\": \"phylum\",\n",
    "        \"class_x\": \"class\",\n",
    "        \"order_x\": \"order\",\n",
    "        \"family_x\": \"family\",\n",
    "        \"genus_x\": \"genus\",\n",
    "        \"unique_id_x\": \"unique_id\",\n",
    "        \"rights_x\": \"rights\",\n",
    "    },\n",
    "    inplace=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "left = left[\n",
    "    [\n",
    "        \"kingdom\",\n",
    "        \"phylum\",\n",
    "        \"class\",\n",
    "        \"order\",\n",
    "        \"family\",\n",
    "        \"genus\",\n",
    "        \"scientific_name\",\n",
    "        \"common_name\",\n",
    "        \"species_type\",\n",
    "        \"unique_id\",\n",
    "        \"rights\",\n",
    "    ]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "left"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "right = outer.loc[\n",
    "    outer._merge == \"right_only\",\n",
    "    [\n",
    "        \"kingdom_y\",\n",
    "        \"phylum_y\",\n",
    "        \"class_y\",\n",
    "        \"order_y\",\n",
    "        \"family_y\",\n",
    "        \"genus_y\",\n",
    "        \"scientific_name\",\n",
    "        \"common_name\",\n",
    "        \"species_type\",\n",
    "        \"unique_id_y\",\n",
    "        \"rights_y\",\n",
    "    ],\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "right.rename(\n",
    "    columns={\n",
    "        \"kingdom_y\": \"kingdom\",\n",
    "        \"phylum_y\": \"phylum\",\n",
    "        \"class_y\": \"class\",\n",
    "        \"order_y\": \"order\",\n",
    "        \"family_y\": \"family\",\n",
    "        \"genus_y\": \"genus\",\n",
    "        \"unique_id_y\": \"unique_id\",\n",
    "        \"rights_y\": \"rights\",\n",
    "    },\n",
    "    inplace=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "right = right[\n",
    "    [\n",
    "        \"kingdom\",\n",
    "        \"phylum\",\n",
    "        \"class\",\n",
    "        \"order\",\n",
    "        \"family\",\n",
    "        \"genus\",\n",
    "        \"scientific_name\",\n",
    "        \"common_name\",\n",
    "        \"species_type\",\n",
    "        \"unique_id\",\n",
    "        \"rights\",\n",
    "    ]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = [inner, left, right]\n",
    "\n",
    "# concatenate dataframes\n",
    "final = pd.concat(frames)\n",
    "\n",
    "# reset index\n",
    "final.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find unique values in scientific_name column of final\n",
    "species_unique = final[\"scientific_name\"].unique()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del frames, left, right, inner, outer, new, old, species_unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read cleaned df into a parquet file where user can input the file path\n",
    "df_merge_path = input(\"Enter the file path: \")\n",
    "\n",
    "\n",
    "# write df to parquet file using pandas to_parquet\n",
    "final.to_parquet(df_merge_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merged = merged.drop([\"unique_id_y\", \"rights_y\", \"_merge\"], axis=1)\n",
    "\n",
    "# merged.rename(columns={\"unique_id_x\": \"unique_id\",\n",
    "#               \"rights_x\": \"rights\"}, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merged\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merged = merged[\n",
    "#     [\n",
    "#         \"kingdom\",\n",
    "#         \"phylum\",\n",
    "#         \"class\",\n",
    "#         \"order\",\n",
    "#         \"family\",\n",
    "#         \"genus\",\n",
    "#         \"scientific_name\",\n",
    "#         \"common_name\",\n",
    "#         \"species_type\",\n",
    "#         \"unique_id\",\n",
    "#         \"rights\",\n",
    "#     ]\n",
    "# ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read cleaned df into a parquet file where user can input the file path\n",
    "df_merge_path = input(\"Enter the file path: \")\n",
    "\n",
    "\n",
    "# write df to parquet file using pandas to_parquet\n",
    "merged.to_parquet(df_merge_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eco",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
