{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "# pd.set_option('display.max_rows', 100)\n",
    "# pd.set_option('display.max_seq_items', 100)\n",
    "\n",
    "# 400 meter coordinate accuracy\n",
    "# 1923 earliest year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in new merged gbif_obis data\n",
    "new_path = input(\"Enter the file path: \")\n",
    "\n",
    "new = pd.read_parquet(new_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in old species data\n",
    "old_path = input(\"Enter the file path: \")\n",
    "\n",
    "# read in the old species data\n",
    "# old = pd.read_csv(old_path)\n",
    "old = pd.read_parquet(old_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# old[\"observed_ecoregions\"].iloc[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only necessary if not already done(if reading parquet its already done)\n",
    "\n",
    "# this has to be done because of bad formatting when downloading from db, might be a better way to download or import. But for now will have to be done for each column that contains a list. THIS IS SLOW, 20+ MIN FOR 300,000+ ROWS\n",
    "for i in range(len(old)):\n",
    "    print(i)\n",
    "    old[\"observed_ecoregions\"].iloc[i] = (\n",
    "        old[\"observed_ecoregions\"]\n",
    "        .iloc[i]\n",
    "        .replace('[\"', \"\")\n",
    "        .replace('\"]', \"\")\n",
    "        .split('\",\"')\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only necessary if not already done(if reading parquet its already done)\n",
    "\n",
    "# this has to be done because of bad formatting when downloading from db, might be a better way to download or import. But for now will have to be done for each column that contains a list. THIS IS SLOW, 20+ MIN FOR 300,000+ ROWS\n",
    "\n",
    "for i in range(len(old)):\n",
    "    print(i)\n",
    "    # handle missing values (missing values are type float)\n",
    "    if type(old[\"native_ecoregions\"].iloc[i]) == str:\n",
    "        old[\"native_ecoregions\"].iloc[i] = (\n",
    "            old[\"native_ecoregions\"]\n",
    "            .iloc[i]\n",
    "            .replace('[\"', \"\")\n",
    "            .replace('\"]', \"\")\n",
    "            .split('\",\"')\n",
    "        )\n",
    "    else:\n",
    "        print(\"missing\")\n",
    "        old[\"native_ecoregions\"].iloc[i] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only necessary if not already done(if reading parquet its already done)\n",
    "\n",
    "old.to_parquet(\n",
    "    \"/media/muskrat/T7 Shield/eco_data/v4/db_exports/species_formatted.parquet\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_main = old[\n",
    "    [\n",
    "        \"kingdom\",\n",
    "        \"phylum\",\n",
    "        \"class\",\n",
    "        \"order\",\n",
    "        \"family\",\n",
    "        \"genus\",\n",
    "        \"scientific_name\",\n",
    "        \"observed_ecoregions\",\n",
    "        # \"rights\",\n",
    "    ]\n",
    "]\n",
    "\n",
    "old_main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create column of old_main called rights where value of every row equals [None]\n",
    "old_main[\"rights\"] = [None] * len(old_main)\n",
    "\n",
    "# need to make placeholders in rights for each ecoregion\n",
    "for i in range(len(old_main)):\n",
    "    eco_length = len(old_main[\"observed_ecoregions\"].iloc[i])\n",
    "    # old_main[\"rights\"].iloc[i] equals array of None with length of eco_length\n",
    "    old_main[\"rights\"].iloc[i] = [None] * eco_length\n",
    "\n",
    "old_main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "side_piece = old[\n",
    "    [\n",
    "        \"kingdom\",\n",
    "        \"phylum\",\n",
    "        \"class\",\n",
    "        \"order\",\n",
    "        \"family\",\n",
    "        \"genus\",\n",
    "        \"scientific_name\",\n",
    "        \"common_name\",\n",
    "        \"species_type\",\n",
    "        \"native_ecoregions\",\n",
    "    ]\n",
    "]\n",
    "\n",
    "side_piece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined = pd.concat([new, old_main], axis=0)\n",
    "\n",
    "combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del new, old, old_main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_grouped = combined.groupby(\n",
    "    [\"scientific_name\", \"kingdom\", \"phylum\", \"class\", \"order\", \"family\", \"genus\"],\n",
    "    as_index=False,\n",
    "    dropna=False,\n",
    ").apply(\n",
    "    lambda x: pd.Series(\n",
    "        {\n",
    "            \"observed_ecoregions\": list(\n",
    "                [item for sublist in x[\"observed_ecoregions\"] for item in sublist]\n",
    "            ),\n",
    "            \"rights\": list([item for sublist in x[\"rights\"] for item in sublist]),\n",
    "        }\n",
    "    )\n",
    ")\n",
    "\n",
    "# combined_grouped = combined.groupby(\n",
    "#     [\"scientific_name\", \"kingdom\", \"phylum\", \"class\", \"order\", \"family\", \"genus\"],\n",
    "#     as_index=False, dropna=False,\n",
    "# ).apply(\n",
    "#     lambda x: pd.Series(\n",
    "#         {\n",
    "#             \"observed_ecoregions\": list(\n",
    "#                 set([item for sublist in x[\"observed_ecoregions\"] for item in sublist])\n",
    "#             ),\n",
    "#             \"rights\": list(\n",
    "#                 set(\n",
    "#                     [\n",
    "#                         item\n",
    "#                         for sublist in x[\"rights\"]\n",
    "#                         for item in sublist\n",
    "#                         if item is not None\n",
    "#                     ]\n",
    "#                 )\n",
    "#             ),\n",
    "#         }\n",
    "#     )\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    f'max ecoregions length is {combined_grouped[\"observed_ecoregions\"].str.len().max()}'\n",
    ")\n",
    "print(f'max rights length is {combined_grouped[\"rights\"].str.len().max()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_grouped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SLOW, TOOK ABOUT AN HOUR FOR 300,000+ ROWS\n",
    "for i in range(len(combined_grouped)):\n",
    "    print(f\"index: {i}\")\n",
    "    # print(len(test[\"observed_ecoregions\"].iloc[i]))\n",
    "    cat = []\n",
    "    rights_cat = []\n",
    "    for j in range(len(combined_grouped[\"observed_ecoregions\"].iloc[i])):\n",
    "        if combined_grouped[\"observed_ecoregions\"].iloc[i][j] not in cat:\n",
    "            cat.append(combined_grouped[\"observed_ecoregions\"].iloc[i][j])\n",
    "            rights_cat.append(combined_grouped[\"rights\"].iloc[i][j])\n",
    "        # print(test[\"observed_ecoregions\"].iloc[i][j])\n",
    "        # for k in test[\"observed_ecoregions\"].iloc[i][j]:\n",
    "        # print(k)\n",
    "        # if k not in cat:\n",
    "        #     cat.append(k)\n",
    "    # if len(cat) != len(test[\"observed_ecoregions\"].iloc[i]):\n",
    "    # print(f'index: {i}')\n",
    "\n",
    "    # print(f'observed: {test[\"observed_ecoregions\"].iloc[i]}')\n",
    "    # print(f'cat: {cat}')\n",
    "    if len(combined_grouped[\"rights\"].iloc[i]) != len(\n",
    "        combined_grouped[\"observed_ecoregions\"].iloc[i]\n",
    "    ):\n",
    "        print(f\"index: {i}\")\n",
    "        print(\"rights\")\n",
    "\n",
    "    combined_grouped[\"observed_ecoregions\"].iloc[i] = cat\n",
    "    combined_grouped[\"rights\"].iloc[i] = rights_cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_grouped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_grouped.to_parquet(\n",
    "    \"/media/muskrat/T7 Shield/eco_data/v4/occurences/step_4/processing/step_2/merge/old_new_retry_2_init.parquet\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged = pd.merge(\n",
    "    combined_grouped,\n",
    "    side_piece,\n",
    "    how=\"outer\",\n",
    "    on=[\"scientific_name\", \"kingdom\", \"phylum\", \"class\", \"order\", \"family\", \"genus\"],\n",
    ")\n",
    "\n",
    "merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged.to_parquet(\n",
    "    \"/media/muskrat/T7 Shield/eco_data/v4/occurences/step_4/processing/step_2/merge/old_new_retry_2_final.parquet\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicates = merged[merged.duplicated([\"scientific_name\"], keep=False)]\n",
    "duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OLD CODE BELOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "species = old[\"scientific_name\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for strange x at beginning of species names and remove if necessary\n",
    "\n",
    "# old = old[\n",
    "#     ~old.scientific_name.isin([\"× Heucherella\", \"×Elyhordeum schaackianum\", \"×Elyleymus aleuticus\"])\n",
    "# ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for species names that longer than 2 words and remove if necessary\n",
    "species_values = np.array([len(word.split()) for word in species])\n",
    "\n",
    "# find the max value in species_values\n",
    "max_value = np.max(species_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if value at index i in species_values is greater than 2, add the value at the same index in species_unique to a new list\n",
    "species_2 = []\n",
    "for i in range(len(species_values)):\n",
    "    if species_values[i] > 2:\n",
    "        species_2.append(species[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove values from species that are greater than 2\n",
    "\n",
    "for i in range(len(species_2)):\n",
    "    old = old[old.scientific_name != species_2[i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del species_2, species_values, max_value, i, species"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inner = pd.merge(new, old, how=\"inner\", on=[\"scientific_name\"], indicator=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inner[\"combined_id\"] = inner[\"unique_id_x\"].apply(lambda x: x.tolist()) + inner[\n",
    "    \"unique_id_y\"\n",
    "].apply(lambda x: x.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inner[\"combined_id\"] = inner[\"combined_id\"].apply(set).apply(list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_combined_id = inner[[\"unique_id_x\", \"unique_id_y\", \"combined_id\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inner = inner.drop([\"unique_id_x\", \"unique_id_y\"], axis=1)\n",
    "\n",
    "inner.rename(columns={\"combined_id\": \"unique_id\"}, inplace=True)\n",
    "\n",
    "del check_combined_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inner[\"combined_rights\"] = inner[\"rights_x\"].apply(lambda x: x.tolist()) + inner[\n",
    "    \"rights_y\"\n",
    "].apply(lambda x: x.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check combined_rights\n",
    "print(\n",
    "    f\"right {inner['rights_x'][0]}, left {inner['rights_y'][0]}, combined {inner['combined_rights'][0]}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove duplicates\n",
    "rights = inner[\"combined_rights\"].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in range(len(rights)):\n",
    "    print(j)\n",
    "    rights[j] = [i for n, i in enumerate(rights[j]) if i not in rights[j][n + 1 :]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rights = pd.Series(rights, name=\"rights\")\n",
    "\n",
    "inner[\"combined_rights\"] = rights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recheck combined_rights\n",
    "print(\n",
    "    f\"right {inner['rights_x'][493]}, left {inner['rights_y'][493]}, combined {inner['combined_rights'][493]}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inner = inner.drop([\"rights_x\", \"rights_y\"], axis=1)\n",
    "\n",
    "inner.rename(columns={\"combined_rights\": \"rights\"}, inplace=True)\n",
    "\n",
    "del rights, j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep columns with less missing values\n",
    "\n",
    "kingdom_x_missing = inner[\"kingdom_x\"].isna().sum()\n",
    "\n",
    "kingdom_y_missing = inner[\"kingdom_y\"].isna().sum()\n",
    "\n",
    "# drop the kingdom column from inner that contains more missing values\n",
    "if kingdom_x_missing > kingdom_y_missing:\n",
    "    inner = inner.drop([\"kingdom_x\"], axis=1)\n",
    "    inner.rename(columns={\"kingdom_y\": \"kingdom\"}, inplace=True)\n",
    "else:\n",
    "    inner = inner.drop([\"kingdom_y\"], axis=1)\n",
    "    inner.rename(columns={\"kingdom_x\": \"kingdom\"}, inplace=True)\n",
    "\n",
    "phylum_x_missing = inner[\"phylum_x\"].isna().sum()\n",
    "\n",
    "phylum_y_missing = inner[\"phylum_y\"].isna().sum()\n",
    "\n",
    "# drop the phylum column from inner that contains more missing values\n",
    "\n",
    "if phylum_x_missing > phylum_y_missing:\n",
    "    inner = inner.drop([\"phylum_x\"], axis=1)\n",
    "    inner.rename(columns={\"phylum_y\": \"phylum\"}, inplace=True)\n",
    "else:\n",
    "    inner = inner.drop([\"phylum_y\"], axis=1)\n",
    "    inner.rename(columns={\"phylum_x\": \"phylum\"}, inplace=True)\n",
    "\n",
    "# find sum of missing values in inner['class_x']\n",
    "class_x_missing = inner[\"class_x\"].isna().sum()\n",
    "\n",
    "class_y_missing = inner[\"class_y\"].isna().sum()\n",
    "\n",
    "# drop the class column from inner that contains more missing values\n",
    "if class_x_missing > class_y_missing:\n",
    "    inner = inner.drop([\"class_x\"], axis=1)\n",
    "    inner.rename(columns={\"class_y\": \"class\"}, inplace=True)\n",
    "else:\n",
    "    inner = inner.drop([\"class_y\"], axis=1)\n",
    "    inner.rename(columns={\"class_x\": \"class\"}, inplace=True)\n",
    "\n",
    "# check\n",
    "class_missing = inner[\"class\"].isna().sum()\n",
    "\n",
    "order_x_missing = inner[\"order_x\"].isna().sum()\n",
    "\n",
    "order_y_missing = inner[\"order_y\"].isna().sum()\n",
    "\n",
    "# drop the order column from inner that contains more missing values\n",
    "\n",
    "if order_x_missing > order_y_missing:\n",
    "    inner = inner.drop([\"order_x\"], axis=1)\n",
    "    inner.rename(columns={\"order_y\": \"order\"}, inplace=True)\n",
    "else:\n",
    "    inner = inner.drop([\"order_y\"], axis=1)\n",
    "    inner.rename(columns={\"order_x\": \"order\"}, inplace=True)\n",
    "\n",
    "family_x_missing = inner[\"family_x\"].isna().sum()\n",
    "\n",
    "family_y_missing = inner[\"family_y\"].isna().sum()\n",
    "\n",
    "# drop the family column from inner that contains more missing values\n",
    "\n",
    "if family_x_missing > family_y_missing:\n",
    "    inner = inner.drop([\"family_x\"], axis=1)\n",
    "    inner.rename(columns={\"family_y\": \"family\"}, inplace=True)\n",
    "else:\n",
    "    inner = inner.drop([\"family_y\"], axis=1)\n",
    "    inner.rename(columns={\"family_x\": \"family\"}, inplace=True)\n",
    "\n",
    "\n",
    "genus_x_missing = inner[\"genus_x\"].isna().sum()\n",
    "\n",
    "genus_y_missing = inner[\"genus_y\"].isna().sum()\n",
    "\n",
    "# drop the genus column from inner that contains more missing values\n",
    "\n",
    "if genus_x_missing > genus_y_missing:\n",
    "    inner = inner.drop([\"genus_x\"], axis=1)\n",
    "    inner.rename(columns={\"genus_y\": \"genus\"}, inplace=True)\n",
    "else:\n",
    "    inner = inner.drop([\"genus_y\"], axis=1)\n",
    "    inner.rename(columns={\"genus_x\": \"genus\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del (\n",
    "    genus_y_missing,\n",
    "    genus_x_missing,\n",
    "    family_y_missing,\n",
    "    family_x_missing,\n",
    "    order_y_missing,\n",
    "    order_x_missing,\n",
    "    class_y_missing,\n",
    "    class_x_missing,\n",
    "    phylum_y_missing,\n",
    "    phylum_x_missing,\n",
    "    kingdom_y_missing,\n",
    "    kingdom_x_missing,\n",
    "    class_missing,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop _merge column\n",
    "inner = inner.drop(\"_merge\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inner = inner[\n",
    "    [\n",
    "        \"kingdom\",\n",
    "        \"phylum\",\n",
    "        \"class\",\n",
    "        \"order\",\n",
    "        \"family\",\n",
    "        \"genus\",\n",
    "        \"scientific_name\",\n",
    "        \"common_name\",\n",
    "        \"species_type\",\n",
    "        \"unique_id\",\n",
    "        \"rights\",\n",
    "    ]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outer = pd.merge(new, old, how=\"outer\", on=[\"scientific_name\"], indicator=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "left = outer.loc[\n",
    "    outer._merge == \"left_only\",\n",
    "    [\n",
    "        \"kingdom_x\",\n",
    "        \"phylum_x\",\n",
    "        \"class_x\",\n",
    "        \"order_x\",\n",
    "        \"family_x\",\n",
    "        \"genus_x\",\n",
    "        \"scientific_name\",\n",
    "        \"common_name\",\n",
    "        \"species_type\",\n",
    "        \"unique_id_x\",\n",
    "        \"rights_x\",\n",
    "    ],\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "left.rename(\n",
    "    columns={\n",
    "        \"kingdom_x\": \"kingdom\",\n",
    "        \"phylum_x\": \"phylum\",\n",
    "        \"class_x\": \"class\",\n",
    "        \"order_x\": \"order\",\n",
    "        \"family_x\": \"family\",\n",
    "        \"genus_x\": \"genus\",\n",
    "        \"unique_id_x\": \"unique_id\",\n",
    "        \"rights_x\": \"rights\",\n",
    "    },\n",
    "    inplace=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "left = left[\n",
    "    [\n",
    "        \"kingdom\",\n",
    "        \"phylum\",\n",
    "        \"class\",\n",
    "        \"order\",\n",
    "        \"family\",\n",
    "        \"genus\",\n",
    "        \"scientific_name\",\n",
    "        \"common_name\",\n",
    "        \"species_type\",\n",
    "        \"unique_id\",\n",
    "        \"rights\",\n",
    "    ]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "left"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "right = outer.loc[\n",
    "    outer._merge == \"right_only\",\n",
    "    [\n",
    "        \"kingdom_y\",\n",
    "        \"phylum_y\",\n",
    "        \"class_y\",\n",
    "        \"order_y\",\n",
    "        \"family_y\",\n",
    "        \"genus_y\",\n",
    "        \"scientific_name\",\n",
    "        \"common_name\",\n",
    "        \"species_type\",\n",
    "        \"unique_id_y\",\n",
    "        \"rights_y\",\n",
    "    ],\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "right.rename(\n",
    "    columns={\n",
    "        \"kingdom_y\": \"kingdom\",\n",
    "        \"phylum_y\": \"phylum\",\n",
    "        \"class_y\": \"class\",\n",
    "        \"order_y\": \"order\",\n",
    "        \"family_y\": \"family\",\n",
    "        \"genus_y\": \"genus\",\n",
    "        \"unique_id_y\": \"unique_id\",\n",
    "        \"rights_y\": \"rights\",\n",
    "    },\n",
    "    inplace=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "right = right[\n",
    "    [\n",
    "        \"kingdom\",\n",
    "        \"phylum\",\n",
    "        \"class\",\n",
    "        \"order\",\n",
    "        \"family\",\n",
    "        \"genus\",\n",
    "        \"scientific_name\",\n",
    "        \"common_name\",\n",
    "        \"species_type\",\n",
    "        \"unique_id\",\n",
    "        \"rights\",\n",
    "    ]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = [inner, left, right]\n",
    "\n",
    "# concatenate dataframes\n",
    "final = pd.concat(frames)\n",
    "\n",
    "# reset index\n",
    "final.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find unique values in scientific_name column of final\n",
    "species_unique = final[\"scientific_name\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del frames, left, right, inner, outer, new, old, species_unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read cleaned df into a parquet file where user can input the file path\n",
    "df_merge_path = input(\"Enter the file path: \")\n",
    "\n",
    "\n",
    "# write df to parquet file using pandas to_parquet\n",
    "final.to_parquet(df_merge_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merged = merged.drop([\"unique_id_y\", \"rights_y\", \"_merge\"], axis=1)\n",
    "\n",
    "# merged.rename(columns={\"unique_id_x\": \"unique_id\",\n",
    "#               \"rights_x\": \"rights\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merged = merged[\n",
    "#     [\n",
    "#         \"kingdom\",\n",
    "#         \"phylum\",\n",
    "#         \"class\",\n",
    "#         \"order\",\n",
    "#         \"family\",\n",
    "#         \"genus\",\n",
    "#         \"scientific_name\",\n",
    "#         \"common_name\",\n",
    "#         \"species_type\",\n",
    "#         \"unique_id\",\n",
    "#         \"rights\",\n",
    "#     ]\n",
    "# ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read cleaned df into a parquet file where user can input the file path\n",
    "df_merge_path = input(\"Enter the file path: \")\n",
    "\n",
    "\n",
    "# write df to parquet file using pandas to_parquet\n",
    "merged.to_parquet(df_merge_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eco",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
