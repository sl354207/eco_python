{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_input = input(\"Enter the meta file path: \")\n",
    "\n",
    "metadata = pd.read_csv(\n",
    "    meta_input,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata.head(40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create new dataframe with latin names, common names, and shp/* columns from metadata\n",
    "\n",
    "metadata = metadata[[\"Latin Name\", \"Common Name\", \"SHP/*\"]]\n",
    "\n",
    "metadata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = input(\"Enter the base path: \")\n",
    "\n",
    "# find all the file names in the directory\n",
    "# files = os.listdir(base_path)\n",
    "files = os.listdir(base_path)\n",
    "\n",
    "\n",
    "# read each file into a dataframe\n",
    "check_init = pd.DataFrame()\n",
    "for file in files:\n",
    "\n",
    "    df = pd.read_parquet(f\"{base_path}/{file}\")\n",
    "    # if unique_ids column = ['test'] then append to check_init dataframe\n",
    "\n",
    "    if df[\"unique_id\"].values[0][0] == \"check\":\n",
    "\n",
    "        check_init = check_init.append(df)\n",
    "\n",
    "check_init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create new dataframe from metadata where scientific_name in check_init == Latin Name in metadata\n",
    "\n",
    "check_meta = metadata[metadata[\"Latin Name\"].isin(check_init[\"scientific_name\"])]\n",
    "\n",
    "check_meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scientific = [\n",
    "    \"Alnus maritima\",\n",
    "    \"Ceanothus spinosus\",\n",
    "    \"Celtis lindheimeri\",\n",
    "    \"Crataegus erythropoda\",\n",
    "    \"Crataegus saligna\",\n",
    "    \"Crataegus tracyi\",\n",
    "    \"Cupressus bakeri\",\n",
    "    \"Cupressus guadalupensis\",\n",
    "    \"Cupressus macrocarpa\",\n",
    "    \"Elliottia racemosa\",\n",
    "    \"Fraxinus gooddingii\",\n",
    "    \"Fraxinus papillosa\",\n",
    "    \"Fremontodendron mexicanum\",\n",
    "    \"Halesia parviflora\",\n",
    "    \"Ilex amelanchier\",\n",
    "    \"Juglans hindsii\",\n",
    "    \"Leitneria floridana\",\n",
    "    \"Nemopanthus collinus\",\n",
    "    \"Ostrya chisosensis\",\n",
    "    \"Ostrya knowltonii\",\n",
    "    \"Pinus balfouriana\",\n",
    "    \"Pinus culminicola\",\n",
    "    \"Pinus nelsonii\",\n",
    "    \"Pinus pinceana\",\n",
    "    \"Pinus washoensis\",\n",
    "    \"Pistacia texana\",\n",
    "    \"Populus hinckleyana\",\n",
    "    \"Prunus alleghaniensis\",\n",
    "    \"Prunus lyonii\",\n",
    "    \"Quercus ajoensis\",\n",
    "    \"Quercus arkansana\",\n",
    "    \"Quercus georgiana\",\n",
    "    \"Quercus graciliformis\",\n",
    "    \"Quercus gravesii\",\n",
    "    \"Quercus hypoleucoides\",\n",
    "]\n",
    "\n",
    "check_meta_2 = metadata[metadata[\"Latin Name\"].isin(scientific)]\n",
    "\n",
    "check_meta_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concat the two dataframes check_meta and check_meta_2\n",
    "\n",
    "check = pd.concat([check_meta, check_meta_2])\n",
    "\n",
    "check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final_1 equals rows in metadata that are not in check\n",
    "\n",
    "final_1 = metadata[~metadata[\"Latin Name\"].isin(check[\"Latin Name\"])]\n",
    "\n",
    "final_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in all the parquet files in the folder where path ends in Latin Name + .parquet of final_1\n",
    "path = input(\"Enter the file path: \")\n",
    "\n",
    "pre_finished = pd.DataFrame()\n",
    "\n",
    "for latin_name in final_1[\"Latin Name\"]:\n",
    "    file = path + latin_name.replace(\" \", \"_\") + \".parquet\"\n",
    "\n",
    "    df = pd.read_parquet(file)\n",
    "    pre_finished = pre_finished.append(df)\n",
    "\n",
    "pre_finished"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_finished.to_parquet(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check.to_parquet(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_input = input(\"Enter the checkfile path: \")\n",
    "\n",
    "check = pd.read_parquet(check_input)\n",
    "\n",
    "check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "little = gpd.read_file(\"\")\n",
    "\n",
    "little"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "little.crs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "little = little.to_crs(\"EPSG:4326\")\n",
    "\n",
    "little.crs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "little.plot().invert_xaxis()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ecomap_input = input(\"Enter the file path: \")\n",
    "\n",
    "eco_map = gpd.read_file(ecomap_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base = eco_map.plot(color=\"white\", edgecolor=\"black\")\n",
    "xmin, ymin, xmax, ymax = (-100, 24, -115, 35)\n",
    "\n",
    "ax = little.plot(ax=base, color=\"red\", alpha=0.4)\n",
    "\n",
    "# set the x and y limits of the plot to the specified bounding box coordinates\n",
    "ax.set_xlim(xmin, xmax)\n",
    "ax.set_ylim(ymin, ymax)\n",
    "ax.invert_xaxis()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intersects = gpd.sjoin(little, eco_map)\n",
    "\n",
    "unique_ids = list(intersects[\"unique_id\"].unique())\n",
    "\n",
    "eco_map_unique = eco_map[eco_map[\"unique_id\"].isin(unique_ids)]\n",
    "\n",
    "# remove rows with <NA> in unique_id from eco_map_unique\n",
    "\n",
    "eco_map_unique = eco_map_unique[eco_map_unique[\"unique_id\"] != \"<NA>\"]\n",
    "\n",
    "# remove rows withe MEOW or PPOW in TYPE from eco_map_unique\n",
    "\n",
    "eco_map_unique = eco_map_unique[~eco_map_unique[\"TYPE\"].isin([\"MEOW\", \"PPOW\"])]\n",
    "\n",
    "overlay = gpd.overlay(little, eco_map, how=\"intersection\")\n",
    "\n",
    "overlay = overlay[overlay[\"unique_id\"] != \"<NA>\"]\n",
    "\n",
    "# remove rows with MEOW or PPOW in TYPE from overlay\n",
    "\n",
    "overlay = overlay[~overlay[\"TYPE\"].isin([\"MEOW\", \"PPOW\"])]\n",
    "\n",
    "overlay.plot(alpha=0.5, edgecolor=\"k\", cmap=\"tab10\")\n",
    "\n",
    "overlay[\"area\"] = overlay.geometry.area\n",
    "\n",
    "overlay_areas = overlay[[\"unique_id\", \"area\"]].groupby(\"unique_id\").sum()\n",
    "\n",
    "eco_map_unique[\"area\"] = eco_map_unique.geometry.area\n",
    "\n",
    "eco_map_unique_areas = eco_map_unique[[\"unique_id\", \"area\"]].groupby(\"unique_id\").sum()\n",
    "\n",
    "combined_areas = pd.concat([overlay_areas, eco_map_unique_areas], axis=1)\n",
    "combined_areas.columns = [\"overlay_area\", \"eco_map_unique_area\"]\n",
    "\n",
    "combined_areas[\"percentage\"] = (\n",
    "    combined_areas[\"overlay_area\"] / combined_areas[\"eco_map_unique_area\"]\n",
    ")\n",
    "\n",
    "combined_areas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "native = [\"310\", \"331\", \"415\"]\n",
    "\n",
    "native"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shp = \"querhypo\"\n",
    "\n",
    "scientific_name = metadata.loc[metadata[\"SHP/*\"] == shp][\"Latin Name\"].values[0]\n",
    "\n",
    "common_name = metadata.loc[metadata[\"SHP/*\"] == shp][\"Common Name\"].values[0]\n",
    "# final dataframe\n",
    "final = pd.DataFrame(\n",
    "    {\n",
    "        \"scientific_name\": scientific_name,\n",
    "        \"common_name\": common_name,\n",
    "        \"unique_id\": native,\n",
    "    },\n",
    ")\n",
    "\n",
    "# groupby scientific_name\n",
    "final = (\n",
    "    final.groupby([\"scientific_name\", \"common_name\"])[\"unique_id\"]\n",
    "    .apply(list)\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "\n",
    "final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "string_name = str(final[\"scientific_name\"].values[0]).replace(\" \", \"_\")\n",
    "\n",
    "final_base_path = input(\"Enter the final base path: \")\n",
    "\n",
    "path = final_base_path + string_name + \".parquet\"\n",
    "\n",
    "final.to_parquet(path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eco",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
